{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import platform\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "import seaborn as sns\n",
    "%precision 3\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "#그래프를 주피터 놋북에 그리기 위해\n",
    "import numpy as np\n",
    "import copy\n",
    "from matplotlib import ticker\n",
    "from scipy.stats import probplot\n",
    "from scipy import stats\n",
    "import collections\n",
    "import itertools\n",
    "import math\n",
    "#from varname import nameof\n",
    "import sys\n",
    "from scipy import stats\n",
    "from scipy.stats import bernoulli\n",
    "import scipy\n",
    "from collections import deque\n",
    "from sympy import Symbol, solve\n",
    "\n",
    "#히스토그램 그리기\n",
    "# Window\n",
    "if platform.system() == 'Windows':\n",
    "    matplotlib.rc('font', family='Malgun Gothic')\n",
    "elif platform.system() == 'Darwin': # Mac\n",
    "    matplotlib.rc('font', family='AppleGothic')\n",
    "else: #linux\n",
    "    matplotlib.rc('font', family='NanumGothic')\n",
    "\n",
    "# 그래프에 마이너스 표시가 되도록 변경\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 한글 폰트 설정\n",
    "font_location = 'C:/Windows/Fonts/MALGUNSL.TTF' #맑은고딕\n",
    "font_name = font_manager.FontProperties(fname=font_location).get_name()\n",
    "rc('font',family=font_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = '../blue_berry/'\n",
    "\n",
    "train = pd.read_csv(data_path + 'train.csv' , index_col = 'id')\n",
    "test = pd.read_csv(data_path + 'test.csv' , index_col = 'id')\n",
    "submission = pd.read_csv(data_path + 'sample_submission.csv' , index_col= 'id')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "       clonesize  honeybee  bumbles  andrena  osmia  MaxOfUpperTRange  \\\nid                                                                      \n0           25.0      0.50     0.25     0.75   0.50              69.7   \n1           25.0      0.50     0.25     0.50   0.50              69.7   \n2           12.5      0.25     0.25     0.63   0.63              86.0   \n3           12.5      0.25     0.25     0.63   0.50              77.4   \n4           25.0      0.50     0.25     0.63   0.63              77.4   \n...          ...       ...      ...      ...    ...               ...   \n15284       12.5      0.25     0.25     0.38   0.50              77.4   \n15285       12.5      0.25     0.25     0.25   0.50              86.0   \n15286       25.0      0.50     0.25     0.38   0.75              77.4   \n15287       25.0      0.50     0.25     0.63   0.63              69.7   \n15288       25.0      0.50     0.25     0.63   0.50              77.4   \n\n       MinOfUpperTRange  AverageOfUpperTRange  MaxOfLowerTRange  \\\nid                                                                \n0                  42.1                  58.2              50.2   \n1                  42.1                  58.2              50.2   \n2                  52.0                  71.9              62.0   \n3                  46.8                  64.7              55.8   \n4                  46.8                  64.7              55.8   \n...                 ...                   ...               ...   \n15284              46.8                  64.7              55.8   \n15285              52.0                  71.9              62.0   \n15286              46.8                  64.7              55.8   \n15287              42.1                  58.2              50.2   \n15288              46.8                  64.7              55.8   \n\n       MinOfLowerTRange  AverageOfLowerTRange  RainingDays  \\\nid                                                           \n0                  24.3                  41.2         24.0   \n1                  24.3                  41.2         24.0   \n2                  30.0                  50.8         24.0   \n3                  27.0                  45.8         24.0   \n4                  27.0                  45.8         24.0   \n...                 ...                   ...          ...   \n15284              27.0                  45.8         16.0   \n15285              30.0                  50.8         34.0   \n15286              27.0                  45.8         34.0   \n15287              24.3                  41.2         24.0   \n15288              27.0                  45.8         16.0   \n\n       AverageRainingDays  fruitset  fruitmass      seeds       yield  \nid                                                                     \n0                    0.39  0.425011   0.417545  32.460887  4476.81146  \n1                    0.39  0.444908   0.422051  33.858317  5548.12201  \n2                    0.39  0.552927   0.470853  38.341781  6869.77760  \n3                    0.39  0.565976   0.478137  39.467561  6880.77590  \n4                    0.39  0.579677   0.494165  40.484512  7479.93417  \n...                   ...       ...        ...        ...         ...  \n15284                0.26  0.556302   0.476308  40.546480  7667.83619  \n15285                0.56  0.354413   0.388145  29.467434  3680.56025  \n15286                0.56  0.422548   0.416786  32.299059  4696.44394  \n15287                0.39  0.542170   0.434133  36.674243  6772.93347  \n15288                0.26  0.492077   0.446576  35.094733  5867.99722  \n\n[15289 rows x 17 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>clonesize</th>\n      <th>honeybee</th>\n      <th>bumbles</th>\n      <th>andrena</th>\n      <th>osmia</th>\n      <th>MaxOfUpperTRange</th>\n      <th>MinOfUpperTRange</th>\n      <th>AverageOfUpperTRange</th>\n      <th>MaxOfLowerTRange</th>\n      <th>MinOfLowerTRange</th>\n      <th>AverageOfLowerTRange</th>\n      <th>RainingDays</th>\n      <th>AverageRainingDays</th>\n      <th>fruitset</th>\n      <th>fruitmass</th>\n      <th>seeds</th>\n      <th>yield</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>25.0</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.75</td>\n      <td>0.50</td>\n      <td>69.7</td>\n      <td>42.1</td>\n      <td>58.2</td>\n      <td>50.2</td>\n      <td>24.3</td>\n      <td>41.2</td>\n      <td>24.0</td>\n      <td>0.39</td>\n      <td>0.425011</td>\n      <td>0.417545</td>\n      <td>32.460887</td>\n      <td>4476.81146</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>25.0</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>69.7</td>\n      <td>42.1</td>\n      <td>58.2</td>\n      <td>50.2</td>\n      <td>24.3</td>\n      <td>41.2</td>\n      <td>24.0</td>\n      <td>0.39</td>\n      <td>0.444908</td>\n      <td>0.422051</td>\n      <td>33.858317</td>\n      <td>5548.12201</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>12.5</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.63</td>\n      <td>0.63</td>\n      <td>86.0</td>\n      <td>52.0</td>\n      <td>71.9</td>\n      <td>62.0</td>\n      <td>30.0</td>\n      <td>50.8</td>\n      <td>24.0</td>\n      <td>0.39</td>\n      <td>0.552927</td>\n      <td>0.470853</td>\n      <td>38.341781</td>\n      <td>6869.77760</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>12.5</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.63</td>\n      <td>0.50</td>\n      <td>77.4</td>\n      <td>46.8</td>\n      <td>64.7</td>\n      <td>55.8</td>\n      <td>27.0</td>\n      <td>45.8</td>\n      <td>24.0</td>\n      <td>0.39</td>\n      <td>0.565976</td>\n      <td>0.478137</td>\n      <td>39.467561</td>\n      <td>6880.77590</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>25.0</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.63</td>\n      <td>0.63</td>\n      <td>77.4</td>\n      <td>46.8</td>\n      <td>64.7</td>\n      <td>55.8</td>\n      <td>27.0</td>\n      <td>45.8</td>\n      <td>24.0</td>\n      <td>0.39</td>\n      <td>0.579677</td>\n      <td>0.494165</td>\n      <td>40.484512</td>\n      <td>7479.93417</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15284</th>\n      <td>12.5</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.38</td>\n      <td>0.50</td>\n      <td>77.4</td>\n      <td>46.8</td>\n      <td>64.7</td>\n      <td>55.8</td>\n      <td>27.0</td>\n      <td>45.8</td>\n      <td>16.0</td>\n      <td>0.26</td>\n      <td>0.556302</td>\n      <td>0.476308</td>\n      <td>40.546480</td>\n      <td>7667.83619</td>\n    </tr>\n    <tr>\n      <th>15285</th>\n      <td>12.5</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>86.0</td>\n      <td>52.0</td>\n      <td>71.9</td>\n      <td>62.0</td>\n      <td>30.0</td>\n      <td>50.8</td>\n      <td>34.0</td>\n      <td>0.56</td>\n      <td>0.354413</td>\n      <td>0.388145</td>\n      <td>29.467434</td>\n      <td>3680.56025</td>\n    </tr>\n    <tr>\n      <th>15286</th>\n      <td>25.0</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.38</td>\n      <td>0.75</td>\n      <td>77.4</td>\n      <td>46.8</td>\n      <td>64.7</td>\n      <td>55.8</td>\n      <td>27.0</td>\n      <td>45.8</td>\n      <td>34.0</td>\n      <td>0.56</td>\n      <td>0.422548</td>\n      <td>0.416786</td>\n      <td>32.299059</td>\n      <td>4696.44394</td>\n    </tr>\n    <tr>\n      <th>15287</th>\n      <td>25.0</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.63</td>\n      <td>0.63</td>\n      <td>69.7</td>\n      <td>42.1</td>\n      <td>58.2</td>\n      <td>50.2</td>\n      <td>24.3</td>\n      <td>41.2</td>\n      <td>24.0</td>\n      <td>0.39</td>\n      <td>0.542170</td>\n      <td>0.434133</td>\n      <td>36.674243</td>\n      <td>6772.93347</td>\n    </tr>\n    <tr>\n      <th>15288</th>\n      <td>25.0</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.63</td>\n      <td>0.50</td>\n      <td>77.4</td>\n      <td>46.8</td>\n      <td>64.7</td>\n      <td>55.8</td>\n      <td>27.0</td>\n      <td>45.8</td>\n      <td>16.0</td>\n      <td>0.26</td>\n      <td>0.492077</td>\n      <td>0.446576</td>\n      <td>35.094733</td>\n      <td>5867.99722</td>\n    </tr>\n  </tbody>\n</table>\n<p>15289 rows × 17 columns</p>\n</div>"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15289 entries, 0 to 15288\n",
      "Data columns (total 17 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   clonesize             15289 non-null  float64\n",
      " 1   honeybee              15289 non-null  float64\n",
      " 2   bumbles               15289 non-null  float64\n",
      " 3   andrena               15289 non-null  float64\n",
      " 4   osmia                 15289 non-null  float64\n",
      " 5   MaxOfUpperTRange      15289 non-null  float64\n",
      " 6   MinOfUpperTRange      15289 non-null  float64\n",
      " 7   AverageOfUpperTRange  15289 non-null  float64\n",
      " 8   MaxOfLowerTRange      15289 non-null  float64\n",
      " 9   MinOfLowerTRange      15289 non-null  float64\n",
      " 10  AverageOfLowerTRange  15289 non-null  float64\n",
      " 11  RainingDays           15289 non-null  float64\n",
      " 12  AverageRainingDays    15289 non-null  float64\n",
      " 13  fruitset              15289 non-null  float64\n",
      " 14  fruitmass             15289 non-null  float64\n",
      " 15  seeds                 15289 non-null  float64\n",
      " 16  yield                 15289 non-null  float64\n",
      "dtypes: float64(17)\n",
      "memory usage: 2.1 MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "          clonesize      honeybee       bumbles       andrena         osmia  \\\ncount  15289.000000  15289.000000  15289.000000  15289.000000  15289.000000   \nmean      19.704690      0.389314      0.286768      0.492675      0.592355   \nstd        6.595211      0.361643      0.059917      0.148115      0.139489   \nmin       10.000000      0.000000      0.000000      0.000000      0.000000   \n25%       12.500000      0.250000      0.250000      0.380000      0.500000   \n50%       25.000000      0.500000      0.250000      0.500000      0.630000   \n75%       25.000000      0.500000      0.380000      0.630000      0.750000   \nmax       40.000000     18.430000      0.585000      0.750000      0.750000   \n\n       MaxOfUpperTRange  MinOfUpperTRange  AverageOfUpperTRange  \\\ncount      15289.000000      15289.000000          15289.000000   \nmean          82.169887         49.673281             68.656256   \nstd            9.146703          5.546405              7.641807   \nmin           69.700000         39.000000             58.200000   \n25%           77.400000         46.800000             64.700000   \n50%           86.000000         52.000000             71.900000   \n75%           86.000000         52.000000             71.900000   \nmax           94.600000         57.200000             79.000000   \n\n       MaxOfLowerTRange  MinOfLowerTRange  AverageOfLowerTRange   RainingDays  \\\ncount      15289.000000      15289.000000          15289.000000  15289.000000   \nmean          59.229538         28.660553             48.568500     18.660865   \nstd            6.610640          3.195367              5.390545     11.657582   \nmin           50.200000         24.300000             41.200000      1.000000   \n25%           55.800000         27.000000             45.800000     16.000000   \n50%           62.000000         30.000000             50.800000     16.000000   \n75%           62.000000         30.000000             50.800000     24.000000   \nmax           68.200000         33.000000             55.900000     34.000000   \n\n       AverageRainingDays      fruitset     fruitmass         seeds  \\\ncount        15289.000000  15289.000000  15289.000000  15289.000000   \nmean             0.324176      0.502741      0.446553     36.164950   \nstd              0.163905      0.074390      0.037035      4.031087   \nmin              0.060000      0.192732      0.311921     22.079199   \n25%              0.260000      0.458246      0.419216     33.232449   \n50%              0.260000      0.506600      0.446570     36.040675   \n75%              0.390000      0.560445      0.474134     39.158238   \nmax              0.560000      0.652144      0.535660     46.585105   \n\n              yield  \ncount  15289.000000  \nmean    6025.193999  \nstd     1337.056850  \nmin     1945.530610  \n25%     5128.163510  \n50%     6117.475900  \n75%     7019.694380  \nmax     8969.401840  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>clonesize</th>\n      <th>honeybee</th>\n      <th>bumbles</th>\n      <th>andrena</th>\n      <th>osmia</th>\n      <th>MaxOfUpperTRange</th>\n      <th>MinOfUpperTRange</th>\n      <th>AverageOfUpperTRange</th>\n      <th>MaxOfLowerTRange</th>\n      <th>MinOfLowerTRange</th>\n      <th>AverageOfLowerTRange</th>\n      <th>RainingDays</th>\n      <th>AverageRainingDays</th>\n      <th>fruitset</th>\n      <th>fruitmass</th>\n      <th>seeds</th>\n      <th>yield</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>15289.000000</td>\n      <td>15289.000000</td>\n      <td>15289.000000</td>\n      <td>15289.000000</td>\n      <td>15289.000000</td>\n      <td>15289.000000</td>\n      <td>15289.000000</td>\n      <td>15289.000000</td>\n      <td>15289.000000</td>\n      <td>15289.000000</td>\n      <td>15289.000000</td>\n      <td>15289.000000</td>\n      <td>15289.000000</td>\n      <td>15289.000000</td>\n      <td>15289.000000</td>\n      <td>15289.000000</td>\n      <td>15289.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>19.704690</td>\n      <td>0.389314</td>\n      <td>0.286768</td>\n      <td>0.492675</td>\n      <td>0.592355</td>\n      <td>82.169887</td>\n      <td>49.673281</td>\n      <td>68.656256</td>\n      <td>59.229538</td>\n      <td>28.660553</td>\n      <td>48.568500</td>\n      <td>18.660865</td>\n      <td>0.324176</td>\n      <td>0.502741</td>\n      <td>0.446553</td>\n      <td>36.164950</td>\n      <td>6025.193999</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>6.595211</td>\n      <td>0.361643</td>\n      <td>0.059917</td>\n      <td>0.148115</td>\n      <td>0.139489</td>\n      <td>9.146703</td>\n      <td>5.546405</td>\n      <td>7.641807</td>\n      <td>6.610640</td>\n      <td>3.195367</td>\n      <td>5.390545</td>\n      <td>11.657582</td>\n      <td>0.163905</td>\n      <td>0.074390</td>\n      <td>0.037035</td>\n      <td>4.031087</td>\n      <td>1337.056850</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>10.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>69.700000</td>\n      <td>39.000000</td>\n      <td>58.200000</td>\n      <td>50.200000</td>\n      <td>24.300000</td>\n      <td>41.200000</td>\n      <td>1.000000</td>\n      <td>0.060000</td>\n      <td>0.192732</td>\n      <td>0.311921</td>\n      <td>22.079199</td>\n      <td>1945.530610</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>12.500000</td>\n      <td>0.250000</td>\n      <td>0.250000</td>\n      <td>0.380000</td>\n      <td>0.500000</td>\n      <td>77.400000</td>\n      <td>46.800000</td>\n      <td>64.700000</td>\n      <td>55.800000</td>\n      <td>27.000000</td>\n      <td>45.800000</td>\n      <td>16.000000</td>\n      <td>0.260000</td>\n      <td>0.458246</td>\n      <td>0.419216</td>\n      <td>33.232449</td>\n      <td>5128.163510</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>25.000000</td>\n      <td>0.500000</td>\n      <td>0.250000</td>\n      <td>0.500000</td>\n      <td>0.630000</td>\n      <td>86.000000</td>\n      <td>52.000000</td>\n      <td>71.900000</td>\n      <td>62.000000</td>\n      <td>30.000000</td>\n      <td>50.800000</td>\n      <td>16.000000</td>\n      <td>0.260000</td>\n      <td>0.506600</td>\n      <td>0.446570</td>\n      <td>36.040675</td>\n      <td>6117.475900</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>25.000000</td>\n      <td>0.500000</td>\n      <td>0.380000</td>\n      <td>0.630000</td>\n      <td>0.750000</td>\n      <td>86.000000</td>\n      <td>52.000000</td>\n      <td>71.900000</td>\n      <td>62.000000</td>\n      <td>30.000000</td>\n      <td>50.800000</td>\n      <td>24.000000</td>\n      <td>0.390000</td>\n      <td>0.560445</td>\n      <td>0.474134</td>\n      <td>39.158238</td>\n      <td>7019.694380</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>40.000000</td>\n      <td>18.430000</td>\n      <td>0.585000</td>\n      <td>0.750000</td>\n      <td>0.750000</td>\n      <td>94.600000</td>\n      <td>57.200000</td>\n      <td>79.000000</td>\n      <td>68.200000</td>\n      <td>33.000000</td>\n      <td>55.900000</td>\n      <td>34.000000</td>\n      <td>0.560000</td>\n      <td>0.652144</td>\n      <td>0.535660</td>\n      <td>46.585105</td>\n      <td>8969.401840</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 864x864 with 16 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 피처 개수 설정\n",
    "num_features = 16\n",
    "\n",
    "# 박스플롯 그리드 생성\n",
    "fig, axes = plt.subplots(nrows=num_features // 4, ncols=4, figsize=(12, 12))\n",
    "\n",
    "# 각 피처별 박스플롯 그리기\n",
    "for i, col in enumerate(train.columns[:num_features]):\n",
    "    ax = axes[i // 4, i % 4]  # 서브플롯 위치 설정\n",
    "    sns.boxplot(x=train[col], ax=ax)  # 박스플롯 그리기\n",
    "    ax.set_title(col)  # 서브플롯 제목 설정\n",
    "\n",
    "plt.tight_layout()  # 서브플롯 간격 조정\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "       clonesize  honeybee  bumbles  andrena  osmia  MaxOfUpperTRange  \\\nid                                                                      \n15289       25.0      0.25     0.25     0.25   0.25              86.0   \n15290       12.5      0.25     0.25     0.75   0.63              94.6   \n15291       12.5      0.25     0.25     0.63   0.63              86.0   \n15292       25.0      0.50     0.38     0.38   0.63              86.0   \n15293       37.5      0.75     0.25     0.25   0.25              94.6   \n...          ...       ...      ...      ...    ...               ...   \n25478       25.0      0.25     0.25     0.25   0.25              86.0   \n25479       25.0      0.50     0.25     0.50   0.75              77.4   \n25480       25.0      0.50     0.38     0.50   0.50              77.4   \n25481       12.5      0.25     0.25     0.38   0.50              94.6   \n25482       12.5      0.25     0.25     0.50   0.50              77.4   \n\n       MinOfUpperTRange  AverageOfUpperTRange  MaxOfLowerTRange  \\\nid                                                                \n15289              52.0                  71.9              62.0   \n15290              57.2                  79.0              68.2   \n15291              52.0                  71.9              62.0   \n15292              52.0                  71.9              62.0   \n15293              57.2                  79.0              68.2   \n...                 ...                   ...               ...   \n25478              52.0                  71.9              62.0   \n25479              46.8                  64.7              55.8   \n25480              46.8                  64.7              55.8   \n25481              57.2                  79.0              68.2   \n25482              46.8                  64.7              55.8   \n\n       MinOfLowerTRange  AverageOfLowerTRange  RainingDays  \\\nid                                                           \n15289              30.0                  50.8         24.0   \n15290              33.0                  55.9          1.0   \n15291              30.0                  50.8         16.0   \n15292              30.0                  50.8         16.0   \n15293              33.0                  55.9         24.0   \n...                 ...                   ...          ...   \n25478              30.0                  50.8         24.0   \n25479              27.0                  45.8         16.0   \n25480              27.0                  45.8         16.0   \n25481              33.0                  55.9         34.0   \n25482              27.0                  45.8          1.0   \n\n       AverageRainingDays  fruitset  fruitmass      seeds  \nid                                                         \n15289                0.39  0.399367   0.408088  31.394569  \n15290                0.10  0.488048   0.442866  36.846956  \n15291                0.26  0.583379   0.487057  40.037644  \n15292                0.26  0.433014   0.422847  33.116091  \n15293                0.39  0.360996   0.388860  29.558019  \n...                   ...       ...        ...        ...  \n25478                0.39  0.474162   0.437923  34.525258  \n25479                0.26  0.482854   0.440676  35.648221  \n25480                0.26  0.568854   0.463065  37.724724  \n25481                0.56  0.407374   0.409261  31.881847  \n25482                0.10  0.579677   0.486202  40.265408  \n\n[10194 rows x 16 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>clonesize</th>\n      <th>honeybee</th>\n      <th>bumbles</th>\n      <th>andrena</th>\n      <th>osmia</th>\n      <th>MaxOfUpperTRange</th>\n      <th>MinOfUpperTRange</th>\n      <th>AverageOfUpperTRange</th>\n      <th>MaxOfLowerTRange</th>\n      <th>MinOfLowerTRange</th>\n      <th>AverageOfLowerTRange</th>\n      <th>RainingDays</th>\n      <th>AverageRainingDays</th>\n      <th>fruitset</th>\n      <th>fruitmass</th>\n      <th>seeds</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>15289</th>\n      <td>25.0</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>86.0</td>\n      <td>52.0</td>\n      <td>71.9</td>\n      <td>62.0</td>\n      <td>30.0</td>\n      <td>50.8</td>\n      <td>24.0</td>\n      <td>0.39</td>\n      <td>0.399367</td>\n      <td>0.408088</td>\n      <td>31.394569</td>\n    </tr>\n    <tr>\n      <th>15290</th>\n      <td>12.5</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.75</td>\n      <td>0.63</td>\n      <td>94.6</td>\n      <td>57.2</td>\n      <td>79.0</td>\n      <td>68.2</td>\n      <td>33.0</td>\n      <td>55.9</td>\n      <td>1.0</td>\n      <td>0.10</td>\n      <td>0.488048</td>\n      <td>0.442866</td>\n      <td>36.846956</td>\n    </tr>\n    <tr>\n      <th>15291</th>\n      <td>12.5</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.63</td>\n      <td>0.63</td>\n      <td>86.0</td>\n      <td>52.0</td>\n      <td>71.9</td>\n      <td>62.0</td>\n      <td>30.0</td>\n      <td>50.8</td>\n      <td>16.0</td>\n      <td>0.26</td>\n      <td>0.583379</td>\n      <td>0.487057</td>\n      <td>40.037644</td>\n    </tr>\n    <tr>\n      <th>15292</th>\n      <td>25.0</td>\n      <td>0.50</td>\n      <td>0.38</td>\n      <td>0.38</td>\n      <td>0.63</td>\n      <td>86.0</td>\n      <td>52.0</td>\n      <td>71.9</td>\n      <td>62.0</td>\n      <td>30.0</td>\n      <td>50.8</td>\n      <td>16.0</td>\n      <td>0.26</td>\n      <td>0.433014</td>\n      <td>0.422847</td>\n      <td>33.116091</td>\n    </tr>\n    <tr>\n      <th>15293</th>\n      <td>37.5</td>\n      <td>0.75</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>94.6</td>\n      <td>57.2</td>\n      <td>79.0</td>\n      <td>68.2</td>\n      <td>33.0</td>\n      <td>55.9</td>\n      <td>24.0</td>\n      <td>0.39</td>\n      <td>0.360996</td>\n      <td>0.388860</td>\n      <td>29.558019</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>25478</th>\n      <td>25.0</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>86.0</td>\n      <td>52.0</td>\n      <td>71.9</td>\n      <td>62.0</td>\n      <td>30.0</td>\n      <td>50.8</td>\n      <td>24.0</td>\n      <td>0.39</td>\n      <td>0.474162</td>\n      <td>0.437923</td>\n      <td>34.525258</td>\n    </tr>\n    <tr>\n      <th>25479</th>\n      <td>25.0</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>77.4</td>\n      <td>46.8</td>\n      <td>64.7</td>\n      <td>55.8</td>\n      <td>27.0</td>\n      <td>45.8</td>\n      <td>16.0</td>\n      <td>0.26</td>\n      <td>0.482854</td>\n      <td>0.440676</td>\n      <td>35.648221</td>\n    </tr>\n    <tr>\n      <th>25480</th>\n      <td>25.0</td>\n      <td>0.50</td>\n      <td>0.38</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>77.4</td>\n      <td>46.8</td>\n      <td>64.7</td>\n      <td>55.8</td>\n      <td>27.0</td>\n      <td>45.8</td>\n      <td>16.0</td>\n      <td>0.26</td>\n      <td>0.568854</td>\n      <td>0.463065</td>\n      <td>37.724724</td>\n    </tr>\n    <tr>\n      <th>25481</th>\n      <td>12.5</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.38</td>\n      <td>0.50</td>\n      <td>94.6</td>\n      <td>57.2</td>\n      <td>79.0</td>\n      <td>68.2</td>\n      <td>33.0</td>\n      <td>55.9</td>\n      <td>34.0</td>\n      <td>0.56</td>\n      <td>0.407374</td>\n      <td>0.409261</td>\n      <td>31.881847</td>\n    </tr>\n    <tr>\n      <th>25482</th>\n      <td>12.5</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>77.4</td>\n      <td>46.8</td>\n      <td>64.7</td>\n      <td>55.8</td>\n      <td>27.0</td>\n      <td>45.8</td>\n      <td>1.0</td>\n      <td>0.10</td>\n      <td>0.579677</td>\n      <td>0.486202</td>\n      <td>40.265408</td>\n    </tr>\n  </tbody>\n</table>\n<p>10194 rows × 16 columns</p>\n</div>"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submission"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_data = pd.concat([train,test]) # 훈련 데이터와 테스트 데이터 합치기\n",
    "# all_data = all_data.drop('Survived' , axis = 1) # 타깃값 제거\n",
    "all_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_data = all_data.drop('yield' , axis = 1) # 타깃값 제거\n",
    "\n",
    "all_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corrMat = train[train.columns.tolist()[:-1]].corr()\n",
    "corrMat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "fig.set_size_inches(15,15)\n",
    "\n",
    "sns.heatmap(corrMat , annot =True )\n",
    "ax.set(title='Heatmap of Numerical Data')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Mask for correlation values less than 0.1\n",
    "fig,ax = plt.subplots()\n",
    "fig.set_size_inches(15,15)\n",
    "mask = np.zeros_like(corrMat)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "mask[np.abs(corrMat) < 0.1] = False\n",
    "\n",
    "sns.heatmap(corrMat, annot=True, mask=mask)\n",
    "ax.set(title='Heatmap of Numerical Data')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 베이스라인 모델_랜덤포레스트_회귀"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#\n",
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "X_train= all_data[:num_train] # 0~num_train -1 행\n",
    "X_test = all_data[num_train:] # num_train ~ 마지막 행\n",
    "\n",
    "y_train = train['yield'].values\n",
    "\n",
    "\n",
    "# ########################################\n",
    "# # 데이터 분할\n",
    "# num_train = len(train) # 훈련 데이터 개수\n",
    "#\n",
    "# # 훈련 데이터와 테스트 데이터 나누기\n",
    "#\n",
    "# X= all_data_sprs[:num_train] # 0~num_train -1 행\n",
    "# X_test = all_data_sprs[num_train:] # num_train ~ 마지막 행\n",
    "#\n",
    "# y = train['Survived'].values\n",
    "#\n",
    "#\n",
    "# X_train , X_valid , y_train, y_valid = train_test_split(X,y, test_size = 0.2, random_state=0)\n",
    "y_test = submission['yield'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 선형 회귀 모델 초기화\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# 그리드 서치를 위한 하이퍼파라미터 그리드 준비\n",
    "param_grid_lr = {\n",
    "    'fit_intercept': [True, False],        # 절편 사용 여부\n",
    "    #'normalize': [True, False]             # 특성 정규화 여부\n",
    "    'n_jobs' : [-1]\n",
    "}\n",
    "\n",
    "# 그리드 서치 객체 생성\n",
    "grid_search_lr = GridSearchCV(lr_model, param_grid_lr, cv=5)\n",
    "\n",
    "# 그리드 서치 수행\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 모델 및 파라미터 출력\n",
    "best_model_grid_search_lr = grid_search_lr.best_estimator_\n",
    "best_params_grid_search_lr = grid_search_lr.best_params_\n",
    "print(\"Best Model (Linear Regression):\", best_model_grid_search_lr)\n",
    "print(\"Best Parameters (Linear Regression):\", best_params_grid_search_lr)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_preds = best_model_grid_search_lr.predict(X_test)\n",
    "\n",
    "y_preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#\n",
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "# X_train= all_data_2[:num_train] # 0~num_train -1 행\n",
    "# X_test = all_data_2[num_train:] # num_train ~ 마지막 행\n",
    "#\n",
    "# y_train = train['Survived'].values\n",
    "\n",
    "\n",
    "# ########################################\n",
    "# # 데이터 분할\n",
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "X= all_data[:num_train] # 0~num_train -1 행\n",
    "X_test = all_data[num_train:] # num_train ~ 마지막 행\n",
    "\n",
    "y = train['yield'].values\n",
    "\n",
    "\n",
    "X_train , X_valid , y_train, y_valid = train_test_split(X,y, test_size = 0.2, random_state=0)\n",
    "# y_test = correct['Survived'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 모델 학습 (예시로 Linear Regression 모델 사용)\n",
    "# model = LinearRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# 검증 데이터에 대한 예측 수행\n",
    "y_pred = best_model_grid_search_lr.predict(X_valid)\n",
    "\n",
    "# MAE 계산\n",
    "mae = mean_absolute_error(y_valid, y_pred)\n",
    "print(\"MAE:\", mae)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# 현재 날짜와 시각 구하기\n",
    "now = datetime.datetime.now()\n",
    "date_list = [now.month, now.day, now.hour, now.minute, now.second]\n",
    "# 현재 날짜와 시각 출력하기\n",
    "print(\"현재 날짜 및 시각 : \", date_list)\n",
    "\n",
    "# 제출 파일 생성\n",
    "\n",
    "submission['yield'] = y_preds\n",
    "submission.to_csv(f'submission_linear_{date_list[0]}_{date_list[1]}_{date_list[2]}_{date_list[3]}_{date_list[4]}.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_data.columns.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_data['clonesize']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from scipy.stats import norm\n",
    "# import matplotlib.pyplot as plt\n",
    "#\n",
    "# # 서브플롯을 생성할 크기 설정\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# # sns.countplot(x='clonesize', hue='yield', data=train)\n",
    "# # 피처들의 리스트\n",
    "# features = all_data.columns.tolist()\n",
    "#\n",
    "# # 피처들에 대한 countplot 그리기\n",
    "# for i, feature in enumerate(features):\n",
    "#     plt.subplot(4, 4, i+1)\n",
    "#     sns.histplot(all_data[feature], kde=True, stat='density', color='skyblue', alpha=0.7)\n",
    "#     mu, std = norm.fit(all_data[feature].dropna())\n",
    "#     xmin, xmax = plt.xlim()\n",
    "#     x = np.linspace(xmin, xmax, 100)\n",
    "#     p = norm.pdf(x, mu, std)\n",
    "#     plt.plot(x, p, 'r', linewidth=2)\n",
    "#     plt.title(f'Features distribution (mu={mu:.2f}, std={std:.2f})')\n",
    "#     plt.title(f'{feature} - Survived Countplot')\n",
    "#\n",
    "# # 레이아웃 조정\n",
    "# plt.tight_layout()\n",
    "#\n",
    "# # 그래프 출력\n",
    "# plt.show()\n",
    "#\n",
    "# # [fruitset , fruitmass ,seeds]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sns.histplot(all_data['fruitset'], kde=True, stat='density', color='skyblue', alpha=0.7)\n",
    "# mu, std = norm.fit(all_data['fruitset'].dropna())\n",
    "# xmin, xmax = plt.xlim()\n",
    "# x = np.linspace(xmin, xmax, 100)\n",
    "# p = norm.pdf(x, mu, std)\n",
    "# plt.plot(x, p, 'r', linewidth=2)\n",
    "# plt.title(f'Features distribution (mu={mu:.2f}, std={std:.2f})')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_data['andrena'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import datetime\n",
    "#\n",
    "# # 현재 날짜와 시각 구하기\n",
    "# now = datetime.datetime.now()\n",
    "# date_list = [now.month, now.day, now.hour, now.minute]\n",
    "# # 현재 날짜와 시각 출력하기\n",
    "# print(\"현재 날짜 및 시각 : \", date_list)\n",
    "#\n",
    "# # 제출 파일 생성\n",
    "#\n",
    "# submission['yield'] = y_preds\n",
    "# submission.to_csv(f'submission_{date_list[0]}_{date_list[1]}_{date_list[2]}_{date_list[3]}.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 베이스라인 모델 선형 모델"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 그리드서치 객체 생성\n",
    "\n",
    "- 그리드서치는 하이퍼파라미터의 값을바꿔가며'모델'의 성능을 교차검증으로 '평가'해 최적의 하이퍼파라미터 값을 찾아준다.\n",
    "\n",
    "- 비교 검증해볼 하이퍼파라미터 값 목록\n",
    "\n",
    "- 대상 모델\n",
    "\n",
    "- 교차 검증용 평가 수단(평가 함수)\n",
    "## 회귀 평가지표\n",
    "\n",
    "- 회귀 모델을 훈련해 최적의 회귀계수를 구할 수 있다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#\n",
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "X_train= all_data[:num_train] # 0~num_train -1 행\n",
    "X_test = all_data[num_train:] # num_train ~ 마지막 행\n",
    "\n",
    "y_train = train['yield'].values\n",
    "\n",
    "\n",
    "# ########################################\n",
    "# # 데이터 분할\n",
    "# num_train = len(train) # 훈련 데이터 개수\n",
    "#\n",
    "# # 훈련 데이터와 테스트 데이터 나누기\n",
    "#\n",
    "# X= all_data_sprs[:num_train] # 0~num_train -1 행\n",
    "# X_test = all_data_sprs[num_train:] # num_train ~ 마지막 행\n",
    "#\n",
    "# y = train['Survived'].values\n",
    "#\n",
    "#\n",
    "# X_train , X_valid , y_train, y_valid = train_test_split(X,y, test_size = 0.2, random_state=0)\n",
    "y_test = submission['yield'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "ridge_model = Ridge()\n",
    "\n",
    "# 하이퍼파라미터 값 목록\n",
    "ridge_params = {'max_iter': [30, 50,100,200], 'alpha': [0.01,0.05,0.1, 1] ,\n",
    "                'random_state' : [42],\n",
    "                'solver' : ['auto']}\n",
    "\n",
    "# 교차 검증용 평가 함수(MAE 계산)\n",
    "mae_scorer = metrics.make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "gridsearch_ridge_model = GridSearchCV(estimator=ridge_model,\n",
    "                                      param_grid=ridge_params,\n",
    "                                      scoring=mae_scorer,\n",
    "                                      cv=5)\n",
    "\n",
    "# 그리드 서치 수행\n",
    "gridsearch_ridge_model.fit(X_train, y_train)\n",
    "\n",
    "best_model_grid_search_ridge = gridsearch_ridge_model.best_estimator_\n",
    "best_params_grid_search_ridge = gridsearch_ridge_model.best_params_\n",
    "\n",
    "print(\"Best Model (Ridge Regression):\", best_model_grid_search_ridge)\n",
    "print(\"Best Parameters (Ridge Regression):\", best_params_grid_search_ridge)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 타깃값 1일 확룰 예측\n",
    "\n",
    "y_preds= best_model_grid_search_ridge.predict(X_test)\n",
    "# y_preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_absolute_error\n",
    "#\n",
    "# # 모델 학습 (예시로 Linear Regression 모델 사용)\n",
    "# # model = LinearRegression()\n",
    "# # model.fit(X_train, y_train)\n",
    "#\n",
    "# # 검증 데이터에 대한 예측 수행\n",
    "# # y_pred = best_model_grid_search_ridge.predict(X_valid)\n",
    "#\n",
    "# # MAE 계산\n",
    "# mae = mean_absolute_error(y_test, y_preds)\n",
    "# print(\"MAE:\", mae)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "# X_train= all_data_2[:num_train] # 0~num_train -1 행\n",
    "# X_test = all_data_2[num_train:] # num_train ~ 마지막 행\n",
    "#\n",
    "# y_train = train['Survived'].values\n",
    "\n",
    "\n",
    "# ########################################\n",
    "# # 데이터 분할\n",
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "X= all_data[:num_train] # 0~num_train -1 행\n",
    "X_test = all_data[num_train:] # num_train ~ 마지막 행\n",
    "\n",
    "y = train['yield'].values\n",
    "\n",
    "\n",
    "X_train , X_valid , y_train, y_valid = train_test_split(X,y, test_size = 0.2, random_state=0)\n",
    "# y_test = correct['Survived'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 모델 학습 (예시로 Linear Regression 모델 사용)\n",
    "# model = LinearRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# 검증 데이터에 대한 예측 수행\n",
    "y_pred = best_model_grid_search_ridge.predict(X_valid)\n",
    "\n",
    "# MAE 계산\n",
    "mae = mean_absolute_error(y_valid, y_pred)\n",
    "print(\"MAE:\", mae)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# 현재 날짜와 시각 구하기\n",
    "now = datetime.datetime.now()\n",
    "date_list = [now.month, now.day, now.hour, now.minute]\n",
    "# 현재 날짜와 시각 출력하기\n",
    "print(\"현재 날짜 및 시각 : \", date_list)\n",
    "\n",
    "# 제출 파일 생성\n",
    "\n",
    "submission['yield'] = y_preds\n",
    "submission.to_csv(f'submission_Ridge_{date_list[0]}_{date_list[1]}_{date_list[2]}_{date_list[3]}.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 부스팅 모델 LightGBM 사용해보기"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#\n",
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "X_train= all_data[:num_train] # 0~num_train -1 행\n",
    "X_test = all_data[num_train:] # num_train ~ 마지막 행\n",
    "\n",
    "y_train = train['yield'].values\n",
    "\n",
    "\n",
    "# ########################################\n",
    "# # 데이터 분할\n",
    "# num_train = len(train) # 훈련 데이터 개수\n",
    "#\n",
    "# # 훈련 데이터와 테스트 데이터 나누기\n",
    "#\n",
    "# X= all_data_sprs[:num_train] # 0~num_train -1 행\n",
    "# X_test = all_data_sprs[num_train:] # num_train ~ 마지막 행\n",
    "#\n",
    "# y = train['Survived'].values\n",
    "#\n",
    "#\n",
    "# X_train , X_valid , y_train, y_valid = train_test_split(X,y, test_size = 0.2, random_state=0)\n",
    "y_test = submission['yield'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eval_gini(y_true , y_pred):\n",
    "    # 실제값과 예측값의 크기가 서로 같은지 확인(값이 다르면 오류 발생)\n",
    "    assert y_true.shape == y_pred.shape\n",
    "\n",
    "    n_samples = y_true.shape[0] # 데이터 개수\n",
    "    L_mid = np.linspace(1/ n_samples ,1 , n_samples) # 대각선 값\n",
    "\n",
    "    # 1) 예측값에 대한 지니계수\n",
    "\n",
    "    pred_order = y_true[y_pred.argsort()] # y_pred 크기순으로 y_true 값 정렬\n",
    "    L_pred = np.cumsum(pred_order) / np.sum(pred_order) # 로렌츠 곡선\n",
    "\n",
    "    G_pred = np.sum(L_mid - L_pred) # 예측값에 대한 지니계수\n",
    "\n",
    "    # 2) 예측이 완벽할 때 지니계수\n",
    "\n",
    "    true_order = y_true[y_true.argsort()] # y_true 크기순으로 y_true 값 정렬\n",
    "    L_true = np.cumsum(true_order) / np.sum(true_order) # 로렌츠 곡선\n",
    "    G_true = np.sum(L_mid - L_true) # 예측이 완벽할 때 지니계수\n",
    "\n",
    "    # 정규화된 지니계수\n",
    "    return G_pred / G_true"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! pip install --upgrade pandas"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# LightGBM 모델 생성\n",
    "lgb_model = LGBMRegressor()\n",
    "\n",
    "# 그리드 서치를 위한 하이퍼파라미터 그리드 준비\n",
    "param_grid_lgb = {\n",
    "    'max_depth': [8,10],                        # 트리의 최대 깊이\n",
    "    'learning_rate': [0.04],               # 학습률\n",
    "    'n_estimators': [50, 200],             # 트리의 개수\n",
    "    'min_child_samples': [1, 2],           # 리프 노드에 필요한 최소 샘플 수\n",
    "    'subsample': [0.9],                   # 트리를 학습할 때 사용할 샘플링 비율\n",
    "    'colsample_bytree': [0.8],                # 트리를 학습할 때 사용할 특성의 비율\n",
    "    'reg_alpha': [0.3 ,0.2],                     # L1 정규화 항의 가중치\n",
    "    'reg_lambda': [ 0.4 , 0.5],                         # L2 정규화 항의 가중치\n",
    "    'random_state': [42]                   # 랜덤 시드\n",
    "}\n",
    "\n",
    "# 그리드 서치 객체 생성\n",
    "grid_search_lgb = GridSearchCV(lgb_model, param_grid_lgb, scoring=mae_scorer,cv=10, n_jobs=-1)\n",
    "\n",
    "# 그리드 서치 수행\n",
    "grid_search_lgb.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 모델 및 파라미터 출력\n",
    "best_model_grid_search_lgb = grid_search_lgb.best_estimator_\n",
    "best_params_grid_search_lgb = grid_search_lgb.best_params_\n",
    "print(\"Best Model (LightGBM):\", best_model_grid_search_lgb)\n",
    "print(\"Best Parameters (LightGBM):\", best_params_grid_search_lgb)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 타깃값 1일 확룰 예측\n",
    "\n",
    "y_preds= best_model_grid_search_lgb.predict(X_test)\n",
    "y_preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "# X_train= all_data_2[:num_train] # 0~num_train -1 행\n",
    "# X_test = all_data_2[num_train:] # num_train ~ 마지막 행\n",
    "#\n",
    "# y_train = train['Survived'].values\n",
    "\n",
    "\n",
    "# ########################################\n",
    "# # 데이터 분할\n",
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "X= all_data[:num_train] # 0~num_train -1 행\n",
    "X_test = all_data[num_train:] # num_train ~ 마지막 행\n",
    "\n",
    "y = train['yield'].values\n",
    "\n",
    "\n",
    "X_train , X_valid , y_train, y_valid = train_test_split(X,y, test_size = 0.2, random_state=0)\n",
    "# y_test = correct['Survived'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 모델 학습 (예시로 Linear Regression 모델 사용)\n",
    "# model = LinearRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# 검증 데이터에 대한 예측 수행\n",
    "y_pred = best_model_grid_search_lgb.predict(X_valid)\n",
    "\n",
    "# MAE 계산\n",
    "mae = mean_absolute_error(y_valid, y_pred)\n",
    "print(\"MAE:\", mae)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# 현재 날짜와 시각 구하기\n",
    "now = datetime.datetime.now()\n",
    "date_list = [now.month, now.day, now.hour, now.minute]\n",
    "# 현재 날짜와 시각 출력하기\n",
    "print(\"현재 날짜 및 시각 : \", date_list)\n",
    "\n",
    "# 제출 파일 생성\n",
    "\n",
    "submission['yield'] = y_preds\n",
    "submission.to_csv(f'submission_LGB_{date_list[0]}_{date_list[1]}_{date_list[2]}_{date_list[3]}.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 부스팅 모델 XGBoost 사용해보기"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#\n",
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "X_train= all_data[:num_train] # 0~num_train -1 행\n",
    "X_test = all_data[num_train:] # num_train ~ 마지막 행\n",
    "\n",
    "y_train = train['yield'].values\n",
    "\n",
    "\n",
    "# ########################################\n",
    "# # 데이터 분할\n",
    "# num_train = len(train) # 훈련 데이터 개수\n",
    "#\n",
    "# # 훈련 데이터와 테스트 데이터 나누기\n",
    "#\n",
    "# X= all_data_sprs[:num_train] # 0~num_train -1 행\n",
    "# X_test = all_data_sprs[num_train:] # num_train ~ 마지막 행\n",
    "#\n",
    "# y = train['Survived'].values\n",
    "#\n",
    "#\n",
    "# X_train , X_valid , y_train, y_valid = train_test_split(X,y, test_size = 0.2, random_state=0)\n",
    "y_test = submission['yield'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "# XGBoost 모델 생성\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# 그리드 서치를 위한 하이퍼파라미터 그리드 준비\n",
    "param_grid_xgb = {\n",
    "    # 'max_depth': [3, 5],\n",
    "    # 'learning_rate': [0.01 , 0.05],\n",
    "    # 'n_estimators': [300, 400],\n",
    "    # 'subsample': [1.0 ],  # subsample 비율\n",
    "    # 'colsample_bytree': [0.8, 0.9 ],  # 각 트리에 사용되는 특성(feature)의 비율\n",
    "    # 'gamma': [0 , 0.1],  # 트리 노드를 추가로 분할하기 위한 최소 손실 감소값\n",
    "    # 'reg_alpha': [0.01 ],  # L1 정규화 항의 가중치\n",
    "    # 'reg_lambda': [0]  # L2 정규화 항의 가중치\n",
    "    \"n_estimators\":[30, 50],\n",
    "    \"max_depth\":[3, 4,],\n",
    "    \"n_jobs\" : [-1],\n",
    "    \"learning_rate\":[.3,.2],\n",
    "    \"subsample\":[.8 , 1.0],\n",
    "    # \"colsample_bytree\":[0.8,1],\n",
    "    # \"gamma\":[0,0.1,1,5],\n",
    "    # \"lambda\":[.01,.1,1],\n",
    "    # \"ManchesterUTD\" : [0, 0.2]\n",
    "\n",
    "}\n",
    "\n",
    "# 그리드 서치 객체 생성\n",
    "grid_search_xgb = GridSearchCV(xgb_model, param_grid_xgb,scoring = mae_scorer, cv=10, n_jobs=-1)\n",
    "\n",
    "# 그리드 서치 수행\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 모델 및 파라미터 출력\n",
    "best_model_grid_search_xgb = grid_search_xgb.best_estimator_\n",
    "best_params_grid_search_xgb = grid_search_xgb.best_params_\n",
    "print(\"Best Model (XGBoost):\", best_model_grid_search_xgb)\n",
    "print(\"Best Parameters (XGBoost):\", best_params_grid_search_xgb)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 타깃값 1일 확룰 예측\n",
    "\n",
    "y_preds= best_model_grid_search_xgb.predict(X_test)\n",
    "y_preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "# X_train= all_data_2[:num_train] # 0~num_train -1 행\n",
    "# X_test = all_data_2[num_train:] # num_train ~ 마지막 행\n",
    "#\n",
    "# y_train = train['Survived'].values\n",
    "\n",
    "\n",
    "# ########################################\n",
    "# # 데이터 분할\n",
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "X= all_data[:num_train] # 0~num_train -1 행\n",
    "X_test = all_data[num_train:] # num_train ~ 마지막 행\n",
    "\n",
    "y = train['yield'].values\n",
    "\n",
    "\n",
    "X_train , X_valid , y_train, y_valid = train_test_split(X,y, test_size = 0.2, random_state=0)\n",
    "# y_test = correct['Survived'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 모델 학습 (예시로 Linear Regression 모델 사용)\n",
    "# model = LinearRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# 검증 데이터에 대한 예측 수행\n",
    "y_pred = best_model_grid_search_xgb.predict(X_valid)\n",
    "\n",
    "# MAE 계산\n",
    "mae = mean_absolute_error(y_valid, y_pred)\n",
    "print(\"MAE:\", mae)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# 현재 날짜와 시각 구하기\n",
    "now = datetime.datetime.now()\n",
    "date_list = [now.month, now.day, now.hour, now.minute , now.second]\n",
    "# 현재 날짜와 시각 출력하기\n",
    "print(\"현재 날짜 및 시각 : \", date_list)\n",
    "\n",
    "# 제출 파일 생성\n",
    "\n",
    "submission['yield'] = y_preds\n",
    "submission.to_csv(f'submission_XGB_{date_list[0]}_{date_list[1]}_{date_list[2]}_{date_list[3]}_{date_list[4]}.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 베이지안 최적화를 통한 최적의 하이퍼파라미터 찾아보기_XGBoost"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#\n",
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "X_train= all_data[:num_train] # 0~num_train -1 행\n",
    "X_test = all_data[num_train:] # num_train ~ 마지막 행\n",
    "\n",
    "y_train = train['yield'].values\n",
    "\n",
    "\n",
    "# ########################################\n",
    "# # 데이터 분할\n",
    "# num_train = len(train) # 훈련 데이터 개수\n",
    "#\n",
    "# # 훈련 데이터와 테스트 데이터 나누기\n",
    "#\n",
    "# X= all_data_sprs[:num_train] # 0~num_train -1 행\n",
    "# X_test = all_data_sprs[num_train:] # num_train ~ 마지막 행\n",
    "#\n",
    "# y = train['Survived'].values\n",
    "#\n",
    "#\n",
    "# X_train , X_valid , y_train, y_valid = train_test_split(X,y, test_size = 0.2, random_state=0)\n",
    "y_test = submission['yield'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# XGBoost 모델 생성\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# 베이지안 최적화를 위한 하이퍼파라미터 범위 정의\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': (30, 50),\n",
    "    'max_depth': (3, 4),\n",
    "    'learning_rate': (0.01, 0.3),\n",
    "    'subsample': (0.8, 1.0),\n",
    "}\n",
    "\n",
    "# 베이지안 최적화 객체 생성\n",
    "bayes_search_xgb = BayesSearchCV(xgb_model, param_grid_xgb, scoring='neg_mean_absolute_error', cv=10, n_jobs=-1)\n",
    "\n",
    "# 베이지안 최적화 수행\n",
    "bayes_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 모델 및 파라미터 출력\n",
    "best_model_bayes_xgb = bayes_search_xgb.best_estimator_\n",
    "best_params_bayes_xgb = bayes_search_xgb.best_params_\n",
    "print(\"Best Model (XGBoost - Bayesian):\", best_model_bayes_xgb)\n",
    "print(\"Best Parameters (XGBoost - Bayesian):\", best_params_bayes_xgb)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 타깃값 1일 확룰 예측\n",
    "\n",
    "y_preds= best_model_grid_search_xgb.predict(X_test)\n",
    "y_preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "# X_train= all_data_2[:num_train] # 0~num_train -1 행\n",
    "# X_test = all_data_2[num_train:] # num_train ~ 마지막 행\n",
    "#\n",
    "# y_train = train['Survived'].values\n",
    "\n",
    "\n",
    "# ########################################\n",
    "# # 데이터 분할\n",
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "X= all_data[:num_train] # 0~num_train -1 행\n",
    "X_test = all_data[num_train:] # num_train ~ 마지막 행\n",
    "\n",
    "y = train['yield'].values\n",
    "\n",
    "\n",
    "X_train , X_valid , y_train, y_valid = train_test_split(X,y, test_size = 0.2, random_state=0)\n",
    "# y_test = correct['Survived'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 모델 학습 (예시로 Linear Regression 모델 사용)\n",
    "# model = LinearRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# 검증 데이터에 대한 예측 수행\n",
    "y_pred = best_model_grid_search_xgb.predict(X_valid)\n",
    "\n",
    "# MAE 계산\n",
    "mae = mean_absolute_error(y_valid, y_pred)\n",
    "print(\"MAE:\", mae)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 모델 학습 (예시로 Linear Regression 모델 사용)\n",
    "# model = LinearRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# 검증 데이터에 대한 예측 수행\n",
    "y_pred = best_model_grid_search_xgb.predict(X_valid)\n",
    "\n",
    "# MAE 계산\n",
    "mae = mean_absolute_error(y_valid, y_pred)\n",
    "print(\"MAE:\", mae)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LightGBM를 활용한 MAE_SCORING활용 및 베이지안 최적화를 수행 이후 충화 K폴드 고차 검증으로 타깃값 예측"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "\n",
    "# num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "# X_train= all_data_2[:num_train] # 0~num_train -1 행\n",
    "# X_test = all_data_2[num_train:] # num_train ~ 마지막 행\n",
    "#\n",
    "# y_train = train['Survived'].values\n",
    "\n",
    "\n",
    "# ########################################\n",
    "# # 데이터 분할\n",
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "all_data_sprs = sparse.csr_matrix(all_data)\n",
    "X= all_data_sprs[:num_train] # 0~num_train -1 행\n",
    "X_test = all_data_sprs[num_train:] # num_train ~ 마지막 행\n",
    "\n",
    "y = train['yield'].values\n",
    "\n",
    "\n",
    "\n",
    "# 8:2 비율로 훈련 데이터, 검증 데이터 분리(베이지안 최적화 수행용)\n",
    "\n",
    "X_train , X_valid , y_train , y_valid = train_test_split(X,y, test_size=0.2 , random_state=0)\n",
    "\n",
    "# 베이지안 최적화용 데이터셋\n",
    "\n",
    "bayes_dtrain = lgb.Dataset(X_train , y_train)\n",
    "bayes_dvalid = lgb.Dataset(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 베이지안 최적화를 위한 하이퍼파라미터 범위\n",
    "param_bounds = {'num_leaves' : (30 , 50) , # 개별 트리가 가질 수 있는 최대 말단 노드 개수 , 트리 복잡도 결정, 값이 클수록 좋다.\n",
    "                'learning_rate' : (0.9, 1), # 학습률( 부스팅 이터레이션을 반복하면서 모델을 업데이트하는 데 사용 되는 비율)\n",
    "                'lambda_l1' : (0.1 , 0.2), # L1 규제 조정값 , 값이 클수록 과대적합 방지 효과\n",
    "                'lambda_l2' : (0.1 , 0.2), # L2 규제 조정값 , 값이 클수록 과대적합 방지 효과\n",
    "                'feature_fraction' : (0.6 , 0.7), # 개별 트리를 훈련할 때 사용할 피처 샘플링 비율\n",
    "                'bagging_fraction' : (0.6 , 0.7), # 개별 트리를 훈련할 때 사용할 데이터 샘플링 비율\n",
    "                'min_child_samples' : (6 , 10) , # 말단 노드가 되기 위해 필요한 최소 데이터 개수 , 값이 클수록 과대적합 방지\n",
    "                'min_child_weight' : (10 , 40), # 과대적합 방지 위한 값\n",
    "                'subsample' : (0.8,1),\n",
    "                }\n",
    "\n",
    "# 값이 고정된 하이퍼파라미터\n",
    "\n",
    "fixed_params = {'objective' : 'regression' , # 훈련 목적 , 회귀에서는 'regression' , 이진분류에서는 'binary' , 다중분류에서는 'multiclass' 사용\n",
    "\n",
    "                'bagging_freq' : 1, # 배깅 수행 빈도, 몇번의 이터레이션마다 배깅 수행할 지 결정\n",
    "                'force_row_wise' : True, # 메모리 용량이 충분하지 않을 때 메모리 효율을 높이는 파라미터\n",
    "                'random_state' : 1991} # 랜덤 시드값 (코드를 반복 실행해도 같은 결과가 나오게 지정하는 값)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gini(preds , dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini' , eval_gini(labels, preds) , True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def mae_scorer(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'Mae' , mean_absolute_error(labels, preds), True\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def eval_function(num_leaves , lambda_l1 , lambda_l2 , feature_fraction , bagging_fraction , min_child_samples , min_child_weight) :\n",
    "#\n",
    "#     # 최적화하려는 평가지표(지니계수) 계산 함수\n",
    "#\n",
    "#     # 베이지안 최적화를 수행할 하이퍼파라미터\n",
    "#\n",
    "#     params = {'num_leaves' : int(round(num_leaves)) , # 개발 트리가 가질 수 있는 최대 말단 노드 개수, 트리 복잡도 결정 , 값이 클수록 좋다.\n",
    "#               'lambda_l1' : lambda_l1, # L1 규제 조정값 , 값이 클 수록 과대적합 방지 효과\n",
    "#               'lambda_l2' : lambda_l2 , # L2 규제 조정값 , 값이 클 수록 과대적합 방지 효과\n",
    "#               'feature_fraction' : feature_fraction ,  # 개별 트리를 훈련할 때 사용할 피처 샘플링 비율\n",
    "#               'bagging_fraction' : bagging_fraction, # 개별 트리를 훈련할 때 사용할 배깅 데이터 샘플링 비율\n",
    "#               'min_child_samples' : int(round(min_child_samples)) , # 말단 노드가 되기 위해 필요한 최소 데이터 개수, 값이 클수록 과대적합 방지\n",
    "#               'min_child_weight' : min_child_weight, # 과대적합 방지 위한 값\n",
    "#               'feature_pre_filter' : False} #\n",
    "#\n",
    "#     #하이퍼파라미터도 추가\n",
    "#     params.update(fixed_params)\n",
    "#\n",
    "#     print('하이퍼파라미터 : ' , params)\n",
    "#\n",
    "#     # LightGBM 모델 훈련\n",
    "#     lgb_model = lgb.train(params = params , # 훈련용 하이퍼파라미터\n",
    "#                           train_set = bayes_dtrain, # 훈련 데이터셋\n",
    "#                           num_boost_round= 2500, #부스팅 반복횟수\n",
    "#                           valid_sets= bayes_dvalid, # 성능 평가용 검증 데이터 셋\n",
    "#                           feval = gini, # 검증용 평가지표\n",
    "#                           early_stopping_rounds= 300, # 조기종료 조건\n",
    "#                           verbose_eval= False) # 계속 점수 출력\n",
    "#     # 검증 데이터로 예측 수행\n",
    "#     preds = lgb_model.predict(X_valid)\n",
    "#\n",
    "#     # 지니계수 계산\n",
    "#     gini_score = eval_gini(y_valid, preds)\n",
    "#     print(f'지니계수 : {gini_score}\\n')\n",
    "#\n",
    "#     return gini_score\n",
    "\n",
    "#===========================================================\n",
    "def eval_function(num_leaves,learning_rate, lambda_l1, lambda_l2, feature_fraction, bagging_fraction, min_child_samples, min_child_weight ,subsample):\n",
    "    # 평가지표(MAE) 계산을 위한 함수\n",
    "    params = {\n",
    "        'num_leaves': int(round(num_leaves)),\n",
    "        'learning_rate' : learning_rate,\n",
    "        'lambda_l1': lambda_l1,\n",
    "        'lambda_l2': lambda_l2,\n",
    "        'feature_fraction': feature_fraction,\n",
    "        'bagging_fraction': bagging_fraction,\n",
    "        'min_child_samples': int(round(min_child_samples)),\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'feature_pre_filter': False,\n",
    "        'subsample' : subsample\n",
    "    }\n",
    "    params.update(fixed_params)\n",
    "\n",
    "    print('하이퍼파라미터:', params)\n",
    "\n",
    "    # LightGBM 모델 훈련\n",
    "    lgb_model = lgb.train(\n",
    "        params=params,\n",
    "        train_set=bayes_dtrain,\n",
    "        num_boost_round=2500,\n",
    "        valid_sets=bayes_dvalid,\n",
    "        feval=mae_scorer,  # 수정하지 않음: MAE 평가지표 사용\n",
    "        early_stopping_rounds=300,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    # 검증 데이터로 예측 수행\n",
    "    preds = lgb_model.predict(X_valid)\n",
    "\n",
    "    # MAE 계산\n",
    "    mae = mean_absolute_error(y_valid, preds)\n",
    "    print(f'MAE: {mae}\\n')\n",
    "\n",
    "    return -mae  # 수정하지 않음: 음수로 반환하여 최적화에 활용\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# 베이지안 최적화 객체 생성\n",
    "optimizer = BayesianOptimization(f = eval_function, # 평가지표 계산 함수\n",
    "                                 pbounds = param_bounds, # 하이퍼파라미터 범위\n",
    "                                 random_state = 0 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "# 베이지안 최적화 수행\n",
    "\n",
    "optimizer.maximize(init_points=  3 , n_iter = 6) # init_points 는 무작위로 하이퍼파라미터를 탐색하는 횟수, n_iter는 베이지안 최적화 반복 횟수\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 평가함수 점수가 최대일 대 하이퍼파라미터\n",
    "max_params = optimizer.max['params']\n",
    "max_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 정수형 하이퍼파라미터 변환\n",
    "\n",
    "max_params['num_leaves'] = int(round(max_params['num_leaves']))\n",
    "max_params['min_child_samples'] = int(round(max_params['min_child_samples']))\n",
    "max_params['subsample'] = float(round(max_params['subsample'] ,2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_params.update(fixed_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import StratifiedKFold ,KFold\n",
    "\n",
    "# 층화 K 폴드 교차 검증기 생성\n",
    "folds = KFold(n_splits=5 , shuffle = True , random_state= 1991)\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "\n",
    "oof_val_preds = np.zeros(X.shape[0])\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 테스트 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_test_preds = np.zeros(X_test.shape[0])\n",
    "\n",
    "# OOF 방식으로 모델 훈련 ,검증 , 예측\n",
    "\n",
    "for idx, (train_idx , valid_idx) in enumerate(folds.split(X,y)):\n",
    "    # 각 폴드를 구분하는 문구 출력\n",
    "    print('#'*40 , f'폴드 {idx+1} / 폴드 {folds.n_splits}' , '#'*40)\n",
    "\n",
    "    X_train , y_train = X[train_idx] , y[train_idx] # 훈련용 데이터\n",
    "    X_valid , y_valid = X[valid_idx] , y[valid_idx] # 검증용 데이터\n",
    "\n",
    "    # LightGBM 전용 데이터셋 생성\n",
    "    dtrain = lgb.Dataset(X_train , y_train) # LightGBM 전용 훈련 데이터셋\n",
    "    dvalid = lgb.Dataset(X_valid , y_valid) # LightGBM 전용 검증 데이터셋\n",
    "\n",
    "    # LightGBM 모델 훈련\n",
    "    lgb_model = lgb.train(params = max_params , # 최적 하이퍼파라미터\n",
    "                          train_set = dtrain, # 훈련 데이터 셋\n",
    "                          num_boost_round= 2500, # 부스팅 반복 횟수\n",
    "                          valid_sets= dvalid , # 성능 평가용 검증 데이터셋\n",
    "                          feval = mae_scorer, # 검증용 평가지표\n",
    "                          early_stopping_rounds= 300, # 조기종료 조건\n",
    "                          verbose_eval = 100) # 100 번째 마다 점수 출력\n",
    "\n",
    "    # 테스트 데이터를 활용해 OOF 예측\n",
    "    oof_test_preds += lgb_model.predict(X_test) / folds.n_splits\n",
    "\n",
    "    # 모델 성능 평가를 위한 검증 데이터 타깃값 예측\n",
    "\n",
    "    oof_val_preds[valid_idx] += lgb_model.predict(X_valid)\n",
    "    oof_test_preds_lgb = oof_test_preds\n",
    "    # 검증 데이터 예측 확률에 대한 정규화 지니계수\n",
    "    Mae_score = mean_absolute_error(y_valid, oof_val_preds[valid_idx])\n",
    "    #gini_score = eval_gini(y_valid, oof_val_preds[valid_idx])\n",
    "    print(f'폴드 {idx+1}  MAE SCORE : {Mae_score}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "oof_val_preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "oof_val_preds[valid_idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('OOF 검증 데이터 지니계수 :' , mean_absolute_error(y, oof_val_preds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_preds = oof_test_preds\n",
    "y_preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "# X_train= all_data_2[:num_train] # 0~num_train -1 행\n",
    "# X_test = all_data_2[num_train:] # num_train ~ 마지막 행\n",
    "#\n",
    "# y_train = train['Survived'].values\n",
    "\n",
    "\n",
    "# ########################################\n",
    "# # 데이터 분할\n",
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "X= all_data[:num_train] # 0~num_train -1 행\n",
    "X_test = all_data[num_train:] # num_train ~ 마지막 행\n",
    "\n",
    "y = train['yield'].values\n",
    "\n",
    "\n",
    "X_train , X_valid , y_train, y_valid = train_test_split(X,y, test_size = 0.2, random_state=0)\n",
    "# y_test = correct['Survived'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 모델 학습 (예시로 Linear Regression 모델 사용)\n",
    "# model = LinearRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# 검증 데이터에 대한 예측 수행\n",
    "y_pred = lgb_model.predict(X_valid)\n",
    "\n",
    "# MAE 계산\n",
    "mae = mean_absolute_error(y_valid, y_pred)\n",
    "print(\"MAE:\", mae)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# 현재 날짜와 시각 구하기\n",
    "now = datetime.datetime.now()\n",
    "date_list = [now.month, now.day, now.hour, now.minute , now.second]\n",
    "# 현재 날짜와 시각 출력하기\n",
    "print(\"현재 날짜 및 시각 : \", date_list)\n",
    "\n",
    "# 제출 파일 생성\n",
    "\n",
    "submission['yield'] = y_preds\n",
    "submission.to_csv(f'submission_KFOLD_LGB_{date_list[0]}_{date_list[1]}_{date_list[2]}_{date_list[3]}_{date_list[4]}.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBoost 활용"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def mae_scorer(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'Mae' , mean_absolute_error(labels, preds)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "# num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "# X_train= all_data_2[:num_train] # 0~num_train -1 행\n",
    "# X_test = all_data_2[num_train:] # num_train ~ 마지막 행\n",
    "#\n",
    "# y_train = train['Survived'].values\n",
    "\n",
    "\n",
    "# ########################################\n",
    "# # 데이터 분할\n",
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "all_data_sprs = sparse.csr_matrix(all_data)\n",
    "X= all_data_sprs[:num_train] # 0~num_train -1 행\n",
    "X_test = all_data_sprs[num_train:] # num_train ~ 마지막 행\n",
    "\n",
    "y = train['yield'].values\n",
    "\n",
    "\n",
    "\n",
    "# 8:2 비율로 훈련 데이터, 검증 데이터 분리(베이지안 최적화 수행용)\n",
    "\n",
    "X_train , X_valid , y_train , y_valid = train_test_split(X,y, test_size=0.2 , random_state=0)\n",
    "\n",
    "# 베이지안 최적화용 데이터셋\n",
    "\n",
    "bayes_dtrain = xgb.DMatrix(X_train , y_train)\n",
    "bayes_dvalid = xgb.DMatrix(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 베이지안 최적화를 위한 하이퍼파라미터 범위\n",
    "param_bounds = {'max_depth' : (4 , 10) , # 개별 트리의 최대 깊이, 트리 깊이가 깊을수록 모델이 복잡해지고 과대적합 우려\n",
    "                # 값이 클수록 깊이가 한 단계만 늘어나도 메모리 사용량이 급격히 많아진다.\n",
    "                # 일반적으로 3~10 사이의 값을 주로 사용한다.\n",
    "\n",
    "                'subsample' : (0.5 , 1), # 개별 트리를 훈련할 때 사용할 데이터 샘플링 비율\n",
    "                # 0~1 사이 값으로 설정할 수 있다.\n",
    "                # 0.5 로 설정하면 전체 데이터의 50%를 사용해 트리를 생성\n",
    "\n",
    "                'colsample_bytree' : (0.7 , 1.0), # 개별 트리를 훈련할 때 사용하는 피처 샘플링 비율\n",
    "                # subsample 과 유사한 개념, subsample은 전체 데이터에서 얼마나 샘플링할지 나타내는 비율\n",
    "                # colsample_bytree는 전체 피처에서 얼마나 샘플링할지 나타내는 비율\n",
    "                # 값이 작을수록 과대적합 방지 효과\n",
    "\n",
    "                'min_child_weight' : (5 , 10), # 과대적합 방지위한 값, 값이 클수록 과대적합 방지 효과가 있다.\n",
    "                'gamma' : (8 , 11), # 말단 노드가 분할하기 위한 최소 손실 감소 값\n",
    "                # 소실 감소가 gamma보다 크면 말단 노드를 분할\n",
    "                # 값이 클수록 과대적합 방지 효과가 있다.\n",
    "\n",
    "                'reg_alpha' : (7 , 9) , # L1 규제 조정 값 , 값이 클수록 과대적합 방지 효과\n",
    "                'reg_lambda' : (1.1 , 1.5), # L2 규제 조정값 , 값이 클수록 과대적합 방지 효과\n",
    "                'scale_pos_weight' : (1.4 , 1.6), # 뷸균형 데이터 가중치 조정 값 ,\n",
    "# 타깃값이 불균형할 때 양성 값에 scale_pos_weight 만큼 가중치를 줘서 균형을 맞춤(타깃값 1을 양성 값으로 간주)\n",
    "# 일반적으로 scale_pos_weight 값을 (음성 타깃값 개수 / 양성 타깃값 개수) 로 설정\n",
    "                'learning_rate' : (0.02, 0.1)} # 학습률( 부스팅 스텝을 반복하면서 모델을 업데이트하는 데 사용되는 비율)\n",
    "\n",
    "\n",
    "# 값이 고정된 하이퍼파라미터\n",
    "\n",
    "fixed_params = {'objective' : 'reg:squarederror' ,# 훈련 목적 , binary : logistic( 확률값을 구하는 이진분류)\n",
    "                # reg : squarederror (회귀 문제)\n",
    "                # 소프트맥스 함수를 사용하는 다중분류에서는 multi : softmax 사용\n",
    "                # 확률값을 구하는 다중분류에서는 'multi : softprob' 사용\n",
    "\n",
    "                'random_state' : 1991} # 랜덤 시드값(코드를 반복 실행해도 같은 결과가 나오게 지정하는 값)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "def eval_function(max_depth ,learning_rate, subsample , colsample_bytree , min_child_weight , reg_alpha , gamma , reg_lambda , scale_pos_weight) :\n",
    "\n",
    "    # 최적화하려는 평가지표(지니계수) 계산 함수\n",
    "\n",
    "    # 베이지안 최적화를 수행할 하이퍼파라미터\n",
    "\n",
    "    params = {'max_depth' : int(round(max_depth)) , # 개별 트리의 최대깊이\n",
    "              'learning_rate' : learning_rate,\n",
    "              'subsample' : subsample, # 개별 트리를 훈련할 때 사용할 데이터 샘플링 비율\n",
    "              'colsample_bytree' : colsample_bytree , # 개별 트리를 훈련할때 사용하는 피처 샘플링\n",
    "              'min_child_weight' :  # 과대적합 방지위한 값\n",
    "                  min_child_weight,\n",
    "              'gamma' : gamma, # 말단 노드가 분할하기 위한 최소 손실 감소 값\n",
    "              'reg_alpha' : reg_alpha, # L1 규제 조정값\n",
    "              'reg_lambda' : reg_lambda, # L2 규제 조정값\n",
    "              'scale_pos_weight' : scale_pos_weight} # 불균형 데이터 가중치 조정값\n",
    "\n",
    "    # 값이 고정된 하이퍼파라미터도 추가\n",
    "    params.update(fixed_params)\n",
    "\n",
    "    print('하이퍼파라미터 : ' , params)\n",
    "\n",
    "    # XGBoost 모델 훈련 , train() 메서드의 하이퍼파라미터\n",
    "    xgb_model = xgb.train(params = params , # XGBoost 모델의 하이퍼파라미터 목록 , 딕셔너리 타입으로 전달\n",
    "                          dtrain = bayes_dtrain, # 훈련 데이터셋, xgboost.DMatrix 타입으로 전달\n",
    "                          num_boost_round= 2000, # 부스팅 반복 횟수, 정수형 타입으로 전달\n",
    "                          # num_boost_round 값이 클수록 성능이 좋아질 수 있으나 과대적합의 우려가 있다.\n",
    "                          # num_boost_round 값이 작으면 반복 횟수가 줄어들어 훈련 시간이 짧아진다.\n",
    "                          # 일반적으로 num_boost_round를 늘리면 learning_rate를 줄여야 한다.\n",
    "\n",
    "                          evals = [(bayes_dvalid , ' bayes_dvalid')],\n",
    "                          # 모델 성능 평가용 검증 데이터셋\n",
    "                          # (DMatrix, 문자열) 쌍들을 원소로 갖는 리스트 타입으로 전달, 검증 데이터셋 이름을 원하는 대로 문자열로 정하면 된다.\n",
    "                          maximize = True, # feval 평가지수가 높으면 좋은지 여부\n",
    "                          feval = mae_scorer, # 검증용 평가지표, 사용자 정의 함수 형태\n",
    "                          # evals를 활용해 모델 성능을 검증할 때 사용할 사용자 정의 평가지표 함수\n",
    "                          # 예측값과 실제값을 파라미터로 전달받아, 평가지표명과 평가점수를 반환하는 함수이다.\n",
    "                          early_stopping_rounds= 200,\n",
    "                          # 조기종료 조건\n",
    "                          # 모델은 기본적으로 num_boost_round만큼 훈련을 반복하며, 매 이터레이션마다 evals로 모델 성능을 평가하여 성능이 연속으로\n",
    "                          # 좋아지지 않는다면 훈련을 중단하는데, 훈련 중단에 필요한 최소횟수가 early_stopping_rounds 이다. 즉 , early_stopping_rounds\n",
    "                          # 동안 모델 성능이 좋아지지 않는다면 훈련을 중단한다.\n",
    "\n",
    "                          # 과대적합 방지 효과\n",
    "\n",
    "                          # 조기종료를 적용하기 위해서는 evals 에 검증 데이터가 하나 이상 있어야한다. 또한 evals에 검증 데이터가 여러 개라면 마지막 검증\n",
    "                          # 데이터를 기준으로 조기종료 조건을 적용한다.\n",
    "\n",
    "\n",
    "                          verbose_eval= False) # 성능 점수 로그 설정 값\n",
    "    # True 로 설정하면 매 부스팅 스텝마다 평가점수르 출력\n",
    "    # 출력값이 너무 많아지는 것을 방지하기위해 verbose_eval로 설정\n",
    "\n",
    "    best_iter = xgb_model.best_iteration # 최적 반복횟수\n",
    "    # 검증 데이터로 예측 수행\n",
    "    preds = xgb_model.predict(bayes_dvalid , iteration_range=(0, best_iter))\n",
    "\n",
    "    # 지니계수 계산\n",
    "    mae_score = mean_absolute_error(y_valid, preds)\n",
    "    print(f'Mae_score : {mae_score}\\n')\n",
    "\n",
    "    return -mae_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# 베이지안 최적화 객체 생성\n",
    "\n",
    "optimizer = BayesianOptimization(f= eval_function, pbounds = param_bounds , random_state= 0)\n",
    "\n",
    "\n",
    "# 베이지안 최적화 수행\n",
    "\n",
    "\n",
    "optimizer.maximize(init_points= 3 , n_iter= 6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 평가함수 점수가 최대일 대 하이퍼파라미터\n",
    "max_params = optimizer.max['params']\n",
    "max_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_params['max_depth'] = int(round(max_params['max_depth']))\n",
    "\n",
    "max_params.update(fixed_params)\n",
    "max_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# %%time\n",
    "# from sklearn.model_selection import StratifiedKFold ,KFold\n",
    "#\n",
    "# # 층화 K 폴드 교차 검증기 생성\n",
    "# folds = KFold(n_splits=5 , shuffle = True , random_state= 1991)\n",
    "#\n",
    "# # OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "#\n",
    "# oof_val_preds = np.zeros(X.shape[0])\n",
    "#\n",
    "# # OOF 방식으로 훈련된 모델로 테스트 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "# oof_test_preds = np.zeros(X_test.shape[0])\n",
    "#\n",
    "# # OOF 방식으로 모델 훈련 ,검증 , 예측\n",
    "#\n",
    "# for idx, (train_idx , valid_idx) in enumerate(folds.split(X,y)):\n",
    "#     # 각 폴드를 구분하는 문구 출력\n",
    "#     print('#'*40 , f'폴드 {idx+1} / 폴드 {folds.n_splits}' , '#'*40)\n",
    "#\n",
    "#     X_train , y_train = X[train_idx] , y[train_idx] # 훈련용 데이터\n",
    "#     X_valid , y_valid = X[valid_idx] , y[valid_idx] # 검증용 데이터\n",
    "#\n",
    "#     # LightGBM 전용 데이터셋 생성\n",
    "#     dtrain = lgb.Dataset(X_train , y_train) # LightGBM 전용 훈련 데이터셋\n",
    "#     dvalid = lgb.Dataset(X_valid , y_valid) # LightGBM 전용 검증 데이터셋\n",
    "#\n",
    "#     # LightGBM 모델 훈련\n",
    "#     lgb_model = lgb.train(params = max_params , # 최적 하이퍼파라미터\n",
    "#                           train_set = dtrain, # 훈련 데이터 셋\n",
    "#                           num_boost_round= 2500, # 부스팅 반복 횟수\n",
    "#                           valid_sets= dvalid , # 성능 평가용 검증 데이터셋\n",
    "#                           feval = mae_scorer, # 검증용 평가지표\n",
    "#                           early_stopping_rounds= 300, # 조기종료 조건\n",
    "#                           verbose_eval = 100) # 100 번째 마다 점수 출력\n",
    "#\n",
    "#     # 테스트 데이터를 활용해 OOF 예측\n",
    "#     oof_test_preds += lgb_model.predict(X_test) / folds.n_splits\n",
    "#\n",
    "#     # 모델 성능 평가를 위한 검증 데이터 타깃값 예측\n",
    "#\n",
    "#     oof_val_preds[valid_idx] += lgb_model.predict(X_valid)\n",
    "#     oof_test_preds_lgb = oof_test_preds\n",
    "#     # 검증 데이터 예측 확률에 대한 정규화 지니계수\n",
    "#     Mae_score = mean_absolute_error(y_valid, oof_val_preds[valid_idx])\n",
    "#     #gini_score = eval_gini(y_valid, oof_val_preds[valid_idx])\n",
    "#     print(f'폴드 {idx+1}  MAE SCORE : {Mae_score}\\n')\n",
    "\n",
    "# ============================================================\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "# 층화 K 폴드 교차 검증기 생성\n",
    "folds = KFold(n_splits= 5 , shuffle= True , random_state= 1991)\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_val_preds = np.zeros(X.shape[0])\n",
    "# # OOF 방식으로 훈련된 모델로 테스트 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_test_preds = np.zeros(X_test.shape[0])\n",
    "# OOF 방식으로 훈련된 모델 훈련 , 검증 , 예측\n",
    "\n",
    "for idx , (train_idx , valid_idx) in enumerate(folds.split(X,y)):\n",
    "    # 각 폴드를 구분하는 문구 출력\n",
    "\n",
    "    print('#' *40,  f'폴드 {idx+1} / 폴드 {folds.n_splits}' , '#'*40)\n",
    "\n",
    "\n",
    "    # 훈련용 데이터, 검증용 데이터 설정\n",
    "    X_train , y_train = X[train_idx] , y[train_idx]\n",
    "    X_valid , y_valid = X[valid_idx] , y[valid_idx]\n",
    "\n",
    "    #XGBoost 전용 데이터셋 생성\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train , y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid , y_valid)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "    #XGBoost 모델 훈련\n",
    "    xgb_model = xgb.train(params = max_params,\n",
    "                          dtrain = dtrain,\n",
    "                          num_boost_round = 2000,\n",
    "                          evals = [(dvalid , 'valid')],\n",
    "                          maximize = True,\n",
    "                          feval = mae_scorer,\n",
    "                          early_stopping_rounds = 200,\n",
    "                          verbose_eval = 100)\n",
    "\n",
    "    # 모델 성능이 가장 좋을 때의 부스팅 반복 횟수 저장\n",
    "    best_iter= xgb_model.best_iteration\n",
    "    # 테스트 데이터를 활용해 OOF 예측\n",
    "\n",
    "    oof_test_preds += xgb_model.predict(dtest, iteration_range = (0 , best_iter))/ folds.n_splits\n",
    "\n",
    "    oof_test_preds_xgb = oof_test_preds\n",
    "    # 모델 성능 평가를 위한 검증 데이터 타깃값 예측\n",
    "    oof_val_preds[valid_idx] += xgb_model.predict(dvalid , iteration_range=(0, best_iter))\n",
    "\n",
    "\n",
    "    # 검증 데이터 예측 확률에 대한 정규화 지니계수\n",
    "    mae_score = mean_absolute_error(y_valid , oof_val_preds[valid_idx])\n",
    "    print(f'폴드 {idx+1}  MAE : {mae_score}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "oof_val_preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "oof_val_preds[valid_idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('OOF 검증 데이터 mae :', mean_absolute_error(y, oof_val_preds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_preds = oof_test_preds\n",
    "y_preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#\n",
    "# num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "# X_train= all_data_2[:num_train] # 0~num_train -1 행\n",
    "# X_test = all_data_2[num_train:] # num_train ~ 마지막 행\n",
    "#\n",
    "# y_train = train['Survived'].values\n",
    "\n",
    "\n",
    "# ########################################\n",
    "# # 데이터 분할\n",
    "num_train = len(train)  # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "X = all_data[:num_train]  # 0~num_train -1 행\n",
    "X_test = all_data[num_train:]  # num_train ~ 마지막 행\n",
    "\n",
    "y = train['yield'].values\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# y_test = correct['Survived'].values\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 모델 학습 (예시로 Linear Regression 모델 사용)\n",
    "# model = LinearRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# 검증 데이터에 대한 예측 수행\n",
    "dvalid = xgb.DMatrix(X_valid)\n",
    "\n",
    "y_pred = xgb_model.predict(dvalid)\n",
    "\n",
    "# MAE 계산\n",
    "mae = mean_absolute_error(y_valid, y_pred)\n",
    "print(\"MAE:\", mae)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# 현재 날짜와 시각 구하기\n",
    "now = datetime.datetime.now()\n",
    "date_list = [now.month, now.day, now.hour, now.minute , now.second]\n",
    "# 현재 날짜와 시각 출력하기\n",
    "print(\"현재 날짜 및 시각 : \", date_list)\n",
    "\n",
    "# 제출 파일 생성\n",
    "\n",
    "submission['yield'] = y_preds\n",
    "submission.to_csv(f'submission_KFOLD_XGB_{date_list[0]}_{date_list[1]}_{date_list[2]}_{date_list[3]}_{date_list[4]}.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PCA 처리 이후 모델 돌려보기"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_data = pd.concat([train,test]) # 훈련 데이터와 테스트 데이터 합치기\n",
    "# all_data = all_data.drop('Survived' , axis = 1) # 타깃값 제거\n",
    "all_data = all_data.drop('yield' , axis = 1) # 타깃값 제거\n",
    "\n",
    "all_data\n",
    "\n",
    "# [fruitset , fruitmass ,seeds]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_data_pca = all_data.iloc[:,:-3]\n",
    "all_data_pca"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 변수 간의 상관관계 행렬 계산\n",
    "corr_matrix = all_data_pca.corr()\n",
    "threshold = 0.9 # 상관관계의 임계값 설정\n",
    "high_corr_vars = corr_matrix[abs(corr_matrix) >= threshold].stack().index\n",
    "high_corr_vars = [var[0] for var in high_corr_vars if var[0] != var[1]]\n",
    "# 상관관계 행렬을 기반으로 PCA 수행\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(corr_matrix)\n",
    "\n",
    "# 주성분 변환 적용\n",
    "transformed_data = pca.transform(all_data_pca)\n",
    "\n",
    "# 변환된 데이터로 업데이트\n",
    "all_data['PCA_Component'] = transformed_data\n",
    "\n",
    "# 상관관계가 높은 변수들을 제외한 업데이트된 데이터\n",
    "selected_vars = [col for col in all_data_pca.columns if col not in high_corr_vars]\n",
    "updated_data = all_data_pca[selected_vars]\n",
    "updated_data\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_data_updated = pd.concat([updated_data, all_data[['fruitset' , 'fruitmass' ,'seeds' , 'PCA_Component']]], axis=1)\n",
    "all_data_updated"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "#\n",
    "# # 마지막 3개의 열을 제외한 열 선택\n",
    "# selected_columns = all_data.columns[:-3]\n",
    "# all_data_2 = pd.DataFrame()\n",
    "# # 각 열에 대해 qcut을 사용하여 5개의 범주로 나누기 (중복된 경계값 제거)\n",
    "# for column in selected_columns:\n",
    "#     all_data_2[column + '_Category'] = pd.qcut(all_data[column], q=5, labels=False, duplicates='drop')\n",
    "#\n",
    "# all_data_2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "#\n",
    "# # 'all_data'의 마지막 3개 열 선택\n",
    "# last_3_columns = all_data.iloc[:, -3:]\n",
    "#\n",
    "# # 'all_data_2'와 'last_3_columns' 합치기\n",
    "# all_data_2 = pd.concat([all_data_2, last_3_columns], axis=1)\n",
    "#\n",
    "# all_data_2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def mae_scorer(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'Mae' , mean_absolute_error(labels, preds)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "# num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "# X_train= all_data_2[:num_train] # 0~num_train -1 행\n",
    "# X_test = all_data_2[num_train:] # num_train ~ 마지막 행\n",
    "#\n",
    "# y_train = train['Survived'].values\n",
    "\n",
    "\n",
    "# ########################################\n",
    "# # 데이터 분할\n",
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "all_data_sprs = sparse.csr_matrix(all_data_updated)\n",
    "X= all_data_sprs[:num_train] # 0~num_train -1 행\n",
    "X_test = all_data_sprs[num_train:] # num_train ~ 마지막 행\n",
    "\n",
    "y = train['yield'].values\n",
    "\n",
    "\n",
    "\n",
    "# 8:2 비율로 훈련 데이터, 검증 데이터 분리(베이지안 최적화 수행용)\n",
    "\n",
    "X_train , X_valid , y_train , y_valid = train_test_split(X,y, test_size=0.2 , random_state=0)\n",
    "\n",
    "# 베이지안 최적화용 데이터셋\n",
    "\n",
    "bayes_dtrain = xgb.DMatrix(X_train , y_train)\n",
    "bayes_dvalid = xgb.DMatrix(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 베이지안 최적화를 위한 하이퍼파라미터 범위\n",
    "param_bounds = {'max_depth' : (4 , 10) , # 개별 트리의 최대 깊이, 트리 깊이가 깊을수록 모델이 복잡해지고 과대적합 우려\n",
    "                # 값이 클수록 깊이가 한 단계만 늘어나도 메모리 사용량이 급격히 많아진다.\n",
    "                # 일반적으로 3~10 사이의 값을 주로 사용한다.\n",
    "\n",
    "                'subsample' : (0.5 , 1), # 개별 트리를 훈련할 때 사용할 데이터 샘플링 비율\n",
    "                # 0~1 사이 값으로 설정할 수 있다.\n",
    "                # 0.5 로 설정하면 전체 데이터의 50%를 사용해 트리를 생성\n",
    "\n",
    "                'colsample_bytree' : (0.5 , 1.0), # 개별 트리를 훈련할 때 사용하는 피처 샘플링 비율\n",
    "                # subsample 과 유사한 개념, subsample은 전체 데이터에서 얼마나 샘플링할지 나타내는 비율\n",
    "                # colsample_bytree는 전체 피처에서 얼마나 샘플링할지 나타내는 비율\n",
    "                # 값이 작을수록 과대적합 방지 효과\n",
    "\n",
    "                'min_child_weight' : (5 , 10), # 과대적합 방지위한 값, 값이 클수록 과대적합 방지 효과가 있다.\n",
    "                'gamma' : (8 , 11), # 말단 노드가 분할하기 위한 최소 손실 감소 값\n",
    "                # 소실 감소가 gamma보다 크면 말단 노드를 분할\n",
    "                # 값이 클수록 과대적합 방지 효과가 있다.\n",
    "\n",
    "                'reg_alpha' : (7 , 9) , # L1 규제 조정 값 , 값이 클수록 과대적합 방지 효과\n",
    "                'reg_lambda' : (1.1 , 1.5), # L2 규제 조정값 , 값이 클수록 과대적합 방지 효과\n",
    "                'scale_pos_weight' : (1.4 , 1.6), # 뷸균형 데이터 가중치 조정 값 ,\n",
    "                # 타깃값이 불균형할 때 양성 값에 scale_pos_weight 만큼 가중치를 줘서 균형을 맞춤(타깃값 1을 양성 값으로 간주)\n",
    "                # 일반적으로 scale_pos_weight 값을 (음성 타깃값 개수 / 양성 타깃값 개수) 로 설정\n",
    "                'learning_rate' : (0.02, 0.1)} # 학습률( 부스팅 스텝을 반복하면서 모델을 업데이트하는 데 사용되는 비율)\n",
    "\n",
    "\n",
    "# 값이 고정된 하이퍼파라미터\n",
    "\n",
    "fixed_params = {'objective' : 'reg:squarederror' ,# 훈련 목적 , binary : logistic( 확률값을 구하는 이진분류)\n",
    "                # reg : squarederror (회귀 문제)\n",
    "                # 소프트맥스 함수를 사용하는 다중분류에서는 multi : softmax 사용\n",
    "                # 확률값을 구하는 다중분류에서는 'multi : softprob' 사용\n",
    "\n",
    "                'random_state' : 1991} # 랜덤 시드값(코드를 반복 실행해도 같은 결과가 나오게 지정하는 값)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "def eval_function(max_depth ,learning_rate, subsample , colsample_bytree , min_child_weight , reg_alpha , gamma , reg_lambda , scale_pos_weight) :\n",
    "\n",
    "    # 최적화하려는 평가지표(지니계수) 계산 함수\n",
    "\n",
    "    # 베이지안 최적화를 수행할 하이퍼파라미터\n",
    "\n",
    "    params = {'max_depth' : int(round(max_depth)) , # 개별 트리의 최대깊이\n",
    "              'learning_rate' : learning_rate,\n",
    "              'subsample' : subsample, # 개별 트리를 훈련할 때 사용할 데이터 샘플링 비율\n",
    "              'colsample_bytree' : colsample_bytree , # 개별 트리를 훈련할때 사용하는 피처 샘플링\n",
    "              'min_child_weight' :  # 과대적합 방지위한 값\n",
    "                  min_child_weight,\n",
    "              'gamma' : gamma, # 말단 노드가 분할하기 위한 최소 손실 감소 값\n",
    "              'reg_alpha' : reg_alpha, # L1 규제 조정값\n",
    "              'reg_lambda' : reg_lambda, # L2 규제 조정값\n",
    "              'scale_pos_weight' : scale_pos_weight} # 불균형 데이터 가중치 조정값\n",
    "\n",
    "    # 값이 고정된 하이퍼파라미터도 추가\n",
    "    params.update(fixed_params)\n",
    "\n",
    "    print('하이퍼파라미터 : ' , params)\n",
    "\n",
    "    # XGBoost 모델 훈련 , train() 메서드의 하이퍼파라미터\n",
    "    xgb_model = xgb.train(params = params , # XGBoost 모델의 하이퍼파라미터 목록 , 딕셔너리 타입으로 전달\n",
    "                          dtrain = bayes_dtrain, # 훈련 데이터셋, xgboost.DMatrix 타입으로 전달\n",
    "                          num_boost_round= 2000, # 부스팅 반복 횟수, 정수형 타입으로 전달\n",
    "                          # num_boost_round 값이 클수록 성능이 좋아질 수 있으나 과대적합의 우려가 있다.\n",
    "                          # num_boost_round 값이 작으면 반복 횟수가 줄어들어 훈련 시간이 짧아진다.\n",
    "                          # 일반적으로 num_boost_round를 늘리면 learning_rate를 줄여야 한다.\n",
    "\n",
    "                          evals = [(bayes_dvalid , ' bayes_dvalid')],\n",
    "                          # 모델 성능 평가용 검증 데이터셋\n",
    "                          # (DMatrix, 문자열) 쌍들을 원소로 갖는 리스트 타입으로 전달, 검증 데이터셋 이름을 원하는 대로 문자열로 정하면 된다.\n",
    "                          maximize = True, # feval 평가지수가 높으면 좋은지 여부\n",
    "                          feval = mae_scorer, # 검증용 평가지표, 사용자 정의 함수 형태\n",
    "                          # evals를 활용해 모델 성능을 검증할 때 사용할 사용자 정의 평가지표 함수\n",
    "                          # 예측값과 실제값을 파라미터로 전달받아, 평가지표명과 평가점수를 반환하는 함수이다.\n",
    "                          early_stopping_rounds= 200,\n",
    "                          # 조기종료 조건\n",
    "                          # 모델은 기본적으로 num_boost_round만큼 훈련을 반복하며, 매 이터레이션마다 evals로 모델 성능을 평가하여 성능이 연속으로\n",
    "                          # 좋아지지 않는다면 훈련을 중단하는데, 훈련 중단에 필요한 최소횟수가 early_stopping_rounds 이다. 즉 , early_stopping_rounds\n",
    "                          # 동안 모델 성능이 좋아지지 않는다면 훈련을 중단한다.\n",
    "\n",
    "                          # 과대적합 방지 효과\n",
    "\n",
    "                          # 조기종료를 적용하기 위해서는 evals 에 검증 데이터가 하나 이상 있어야한다. 또한 evals에 검증 데이터가 여러 개라면 마지막 검증\n",
    "                          # 데이터를 기준으로 조기종료 조건을 적용한다.\n",
    "\n",
    "\n",
    "                          verbose_eval= False) # 성능 점수 로그 설정 값\n",
    "    # True 로 설정하면 매 부스팅 스텝마다 평가점수르 출력\n",
    "    # 출력값이 너무 많아지는 것을 방지하기위해 verbose_eval로 설정\n",
    "\n",
    "    best_iter = xgb_model.best_iteration # 최적 반복횟수\n",
    "    # 검증 데이터로 예측 수행\n",
    "    preds = xgb_model.predict(bayes_dvalid , iteration_range=(0, best_iter))\n",
    "\n",
    "    # 지니계수 계산\n",
    "    mae_score = mean_absolute_error(y_valid, preds)\n",
    "    print(f'Mae_score : {mae_score}\\n')\n",
    "\n",
    "    return -mae_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# 베이지안 최적화 객체 생성\n",
    "\n",
    "optimizer = BayesianOptimization(f= eval_function, pbounds = param_bounds , random_state= 0)\n",
    "\n",
    "\n",
    "# 베이지안 최적화 수행\n",
    "\n",
    "\n",
    "optimizer.maximize(init_points= 3 , n_iter= 6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 평가함수 점수가 최대일 대 하이퍼파라미터\n",
    "max_params = optimizer.max['params']\n",
    "max_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_params['max_depth'] = int(round(max_params['max_depth']))\n",
    "\n",
    "max_params.update(fixed_params)\n",
    "max_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "# 층화 K 폴드 교차 검증기 생성\n",
    "folds = KFold(n_splits= 5 , shuffle= True , random_state= 1991)\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_val_preds = np.zeros(X.shape[0])\n",
    "# # OOF 방식으로 훈련된 모델로 테스트 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_test_preds = np.zeros(X_test.shape[0])\n",
    "# OOF 방식으로 훈련된 모델 훈련 , 검증 , 예측\n",
    "\n",
    "for idx , (train_idx , valid_idx) in enumerate(folds.split(X,y)):\n",
    "    # 각 폴드를 구분하는 문구 출력\n",
    "\n",
    "    print('#' *40,  f'폴드 {idx+1} / 폴드 {folds.n_splits}' , '#'*40)\n",
    "\n",
    "\n",
    "    # 훈련용 데이터, 검증용 데이터 설정\n",
    "    X_train , y_train = X[train_idx] , y[train_idx]\n",
    "    X_valid , y_valid = X[valid_idx] , y[valid_idx]\n",
    "\n",
    "    #XGBoost 전용 데이터셋 생성\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train , y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid , y_valid)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "    #XGBoost 모델 훈련\n",
    "    xgb_model = xgb.train(params = max_params,\n",
    "                          dtrain = dtrain,\n",
    "                          num_boost_round = 2000,\n",
    "                          evals = [(dvalid , 'valid')],\n",
    "                          maximize = True,\n",
    "                          feval = mae_scorer,\n",
    "                          early_stopping_rounds = 200,\n",
    "                          verbose_eval = 100)\n",
    "\n",
    "    # 모델 성능이 가장 좋을 때의 부스팅 반복 횟수 저장\n",
    "    best_iter= xgb_model.best_iteration\n",
    "    # 테스트 데이터를 활용해 OOF 예측\n",
    "\n",
    "    oof_test_preds += xgb_model.predict(dtest, iteration_range = (0 , best_iter))/ folds.n_splits\n",
    "\n",
    "    oof_test_preds_xgb = oof_test_preds\n",
    "    # 모델 성능 평가를 위한 검증 데이터 타깃값 예측\n",
    "    oof_val_preds[valid_idx] += xgb_model.predict(dvalid , iteration_range=(0, best_iter))\n",
    "\n",
    "\n",
    "    # 검증 데이터 예측 확률에 대한 정규화 지니계수\n",
    "    mae_score = mean_absolute_error(y_valid , oof_val_preds[valid_idx])\n",
    "    print(f'폴드 {idx+1}  MAE : {mae_score}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('OOF 검증 데이터 mae :', mean_absolute_error(y, oof_val_preds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_preds = oof_test_preds\n",
    "y_preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#\n",
    "# num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "# X_train= all_data_2[:num_train] # 0~num_train -1 행\n",
    "# X_test = all_data_2[num_train:] # num_train ~ 마지막 행\n",
    "#\n",
    "# y_train = train['Survived'].values\n",
    "\n",
    "\n",
    "# ########################################\n",
    "# # 데이터 분할\n",
    "num_train = len(train)  # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "X = all_data_updated[:num_train]  # 0~num_train -1 행\n",
    "X_test = all_data_updated[num_train:]  # num_train ~ 마지막 행\n",
    "\n",
    "y = train['yield'].values\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# y_test = correct['Survived'].values\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 모델 학습 (예시로 Linear Regression 모델 사용)\n",
    "# model = LinearRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# 검증 데이터에 대한 예측 수행\n",
    "dvalid = xgb.DMatrix(X_valid)\n",
    "\n",
    "y_pred = xgb_model.predict(dvalid)\n",
    "\n",
    "# MAE 계산\n",
    "mae = mean_absolute_error(y_valid, y_pred)\n",
    "print(\"MAE:\", mae)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# 현재 날짜와 시각 구하기\n",
    "now = datetime.datetime.now()\n",
    "date_list = [now.month, now.day, now.hour, now.minute , now.second]\n",
    "# 현재 날짜와 시각 출력하기\n",
    "print(\"현재 날짜 및 시각 : \", date_list)\n",
    "\n",
    "# 제출 파일 생성\n",
    "\n",
    "submission['yield'] = y_preds\n",
    "submission.to_csv(f'submission_KFOLD_XGB_PCA_{date_list[0]}_{date_list[1]}_{date_list[2]}_{date_list[3]}_{date_list[4]}.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MCA 처리 이후 모델 돌려보기"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_data = pd.concat([train,test]) # 훈련 데이터와 테스트 데이터 합치기\n",
    "# all_data = all_data.drop('Survived' , axis = 1) # 타깃값 제거\n",
    "all_data = all_data.drop('yield' , axis = 1) # 타깃값 제거\n",
    "\n",
    "all_data\n",
    "\n",
    "# [fruitset , fruitmass ,seeds]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_data_Mca = all_data.iloc[:,:-3]\n",
    "all_data_Mca"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ! pip install --trusted-host pypi.python.org --trusted-host files.pythonhosted.org --trusted-host pypi.org  prince"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from prince import MCA\n",
    "\n",
    "# 변수 간의 상관관계 행렬 계산\n",
    "corr_matrix = all_data_Mca.corr()\n",
    "threshold = 0.9 # 상관관계의 임계값 설정\n",
    "high_corr_vars = corr_matrix[abs(corr_matrix) >= threshold].stack().index\n",
    "high_corr_vars = [var[0] for var in high_corr_vars if var[0] != var[1]]\n",
    "# 상관관계 행렬을 기반으로 MCA 수행\n",
    "mca = MCA(n_components=1)\n",
    "mca.fit(all_data_Mca)\n",
    "\n",
    "# 주성분 변환 적용\n",
    "transformed_data = mca.transform(all_data_Mca)\n",
    "\n",
    "# 변환된 데이터로 업데이트\n",
    "all_data['MCA_Component'] = transformed_data\n",
    "\n",
    "# 상관관계가 높은 변수들을 제외한 업데이트된 데이터\n",
    "selected_vars = [col for col in all_data_Mca.columns if col not in high_corr_vars]\n",
    "updated_data = all_data_Mca[selected_vars]\n",
    "updated_data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_data_updated = pd.concat([updated_data, all_data[['fruitset' , 'fruitmass' ,'seeds' , 'MCA_Component']]], axis=1)\n",
    "all_data_updated"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "# num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "# X_train= all_data_2[:num_train] # 0~num_train -1 행\n",
    "# X_test = all_data_2[num_train:] # num_train ~ 마지막 행\n",
    "#\n",
    "# y_train = train['Survived'].values\n",
    "\n",
    "\n",
    "# ########################################\n",
    "# # 데이터 분할\n",
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "all_data_sprs = sparse.csr_matrix(all_data_updated)\n",
    "X= all_data_sprs[:num_train] # 0~num_train -1 행\n",
    "X_test = all_data_sprs[num_train:] # num_train ~ 마지막 행\n",
    "\n",
    "y = train['yield'].values\n",
    "\n",
    "\n",
    "\n",
    "# 8:2 비율로 훈련 데이터, 검증 데이터 분리(베이지안 최적화 수행용)\n",
    "\n",
    "X_train , X_valid , y_train , y_valid = train_test_split(X,y, test_size=0.2 , random_state=0)\n",
    "\n",
    "# 베이지안 최적화용 데이터셋\n",
    "\n",
    "bayes_dtrain = xgb.DMatrix(X_train , y_train)\n",
    "bayes_dvalid = xgb.DMatrix(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 베이지안 최적화를 위한 하이퍼파라미터 범위\n",
    "param_bounds = {'max_depth' : (4 , 10) , # 개별 트리의 최대 깊이, 트리 깊이가 깊을수록 모델이 복잡해지고 과대적합 우려\n",
    "                # 값이 클수록 깊이가 한 단계만 늘어나도 메모리 사용량이 급격히 많아진다.\n",
    "                # 일반적으로 3~10 사이의 값을 주로 사용한다.\n",
    "\n",
    "                'subsample' : (0.5 , 1), # 개별 트리를 훈련할 때 사용할 데이터 샘플링 비율\n",
    "                # 0~1 사이 값으로 설정할 수 있다.\n",
    "                # 0.5 로 설정하면 전체 데이터의 50%를 사용해 트리를 생성\n",
    "\n",
    "                'colsample_bytree' : (0.5 , 1.0), # 개별 트리를 훈련할 때 사용하는 피처 샘플링 비율\n",
    "                # subsample 과 유사한 개념, subsample은 전체 데이터에서 얼마나 샘플링할지 나타내는 비율\n",
    "                # colsample_bytree는 전체 피처에서 얼마나 샘플링할지 나타내는 비율\n",
    "                # 값이 작을수록 과대적합 방지 효과\n",
    "\n",
    "                'min_child_weight' : (5 , 10), # 과대적합 방지위한 값, 값이 클수록 과대적합 방지 효과가 있다.\n",
    "                'gamma' : (8 , 11), # 말단 노드가 분할하기 위한 최소 손실 감소 값\n",
    "                # 소실 감소가 gamma보다 크면 말단 노드를 분할\n",
    "                # 값이 클수록 과대적합 방지 효과가 있다.\n",
    "\n",
    "                'reg_alpha' : (7 , 9) , # L1 규제 조정 값 , 값이 클수록 과대적합 방지 효과\n",
    "                'reg_lambda' : (1.1 , 1.5), # L2 규제 조정값 , 값이 클수록 과대적합 방지 효과\n",
    "                'scale_pos_weight' : (1.4 , 1.6), # 뷸균형 데이터 가중치 조정 값 ,\n",
    "                # 타깃값이 불균형할 때 양성 값에 scale_pos_weight 만큼 가중치를 줘서 균형을 맞춤(타깃값 1을 양성 값으로 간주)\n",
    "                # 일반적으로 scale_pos_weight 값을 (음성 타깃값 개수 / 양성 타깃값 개수) 로 설정\n",
    "                'learning_rate' : (0.02, 0.1)} # 학습률( 부스팅 스텝을 반복하면서 모델을 업데이트하는 데 사용되는 비율)\n",
    "\n",
    "\n",
    "# 값이 고정된 하이퍼파라미터\n",
    "\n",
    "fixed_params = {'objective' : 'reg:squarederror' ,# 훈련 목적 , binary : logistic( 확률값을 구하는 이진분류)\n",
    "                # reg : squarederror (회귀 문제)\n",
    "                # 소프트맥스 함수를 사용하는 다중분류에서는 multi : softmax 사용\n",
    "                # 확률값을 구하는 다중분류에서는 'multi : softprob' 사용\n",
    "\n",
    "                'random_state' : 1991} # 랜덤 시드값(코드를 반복 실행해도 같은 결과가 나오게 지정하는 값)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# 베이지안 최적화 객체 생성\n",
    "\n",
    "optimizer = BayesianOptimization(f= eval_function, pbounds = param_bounds , random_state= 0)\n",
    "\n",
    "\n",
    "# 베이지안 최적화 수행\n",
    "\n",
    "\n",
    "optimizer.maximize(init_points= 3 , n_iter= 6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 평가함수 점수가 최대일 대 하이퍼파라미터\n",
    "max_params = optimizer.max['params']\n",
    "max_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_params['max_depth'] = int(round(max_params['max_depth']))\n",
    "\n",
    "max_params.update(fixed_params)\n",
    "max_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "# 층화 K 폴드 교차 검증기 생성\n",
    "folds = KFold(n_splits= 5 , shuffle= True , random_state= 1991)\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_val_preds = np.zeros(X.shape[0])\n",
    "# # OOF 방식으로 훈련된 모델로 테스트 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_test_preds = np.zeros(X_test.shape[0])\n",
    "# OOF 방식으로 훈련된 모델 훈련 , 검증 , 예측\n",
    "\n",
    "for idx , (train_idx , valid_idx) in enumerate(folds.split(X,y)):\n",
    "    # 각 폴드를 구분하는 문구 출력\n",
    "\n",
    "    print('#' *40,  f'폴드 {idx+1} / 폴드 {folds.n_splits}' , '#'*40)\n",
    "\n",
    "\n",
    "    # 훈련용 데이터, 검증용 데이터 설정\n",
    "    X_train , y_train = X[train_idx] , y[train_idx]\n",
    "    X_valid , y_valid = X[valid_idx] , y[valid_idx]\n",
    "\n",
    "    #XGBoost 전용 데이터셋 생성\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train , y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid , y_valid)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "    #XGBoost 모델 훈련\n",
    "    xgb_model = xgb.train(params = max_params,\n",
    "                          dtrain = dtrain,\n",
    "                          num_boost_round = 2000,\n",
    "                          evals = [(dvalid , 'valid')],\n",
    "                          maximize = True,\n",
    "                          feval = mae_scorer,\n",
    "                          early_stopping_rounds = 200,\n",
    "                          verbose_eval = 100)\n",
    "\n",
    "    # 모델 성능이 가장 좋을 때의 부스팅 반복 횟수 저장\n",
    "    best_iter= xgb_model.best_iteration\n",
    "    # 테스트 데이터를 활용해 OOF 예측\n",
    "\n",
    "    oof_test_preds += xgb_model.predict(dtest, iteration_range = (0 , best_iter))/ folds.n_splits\n",
    "\n",
    "    oof_test_preds_xgb = oof_test_preds\n",
    "    # 모델 성능 평가를 위한 검증 데이터 타깃값 예측\n",
    "    oof_val_preds[valid_idx] += xgb_model.predict(dvalid , iteration_range=(0, best_iter))\n",
    "\n",
    "\n",
    "    # 검증 데이터 예측 확률에 대한 정규화 지니계수\n",
    "    mae_score = mean_absolute_error(y_valid , oof_val_preds[valid_idx])\n",
    "    print(f'폴드 {idx+1}  MAE : {mae_score}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('OOF 검증 데이터 mae :', mean_absolute_error(y, oof_val_preds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_preds = oof_test_preds\n",
    "y_preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#\n",
    "# num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "# X_train= all_data_2[:num_train] # 0~num_train -1 행\n",
    "# X_test = all_data_2[num_train:] # num_train ~ 마지막 행\n",
    "#\n",
    "# y_train = train['Survived'].values\n",
    "\n",
    "\n",
    "# ########################################\n",
    "# # 데이터 분할\n",
    "num_train = len(train)  # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "X = all_data_updated[:num_train]  # 0~num_train -1 행\n",
    "X_test = all_data_updated[num_train:]  # num_train ~ 마지막 행\n",
    "\n",
    "y = train['yield'].values\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# y_test = correct['Survived'].values\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 모델 학습 (예시로 Linear Regression 모델 사용)\n",
    "# model = LinearRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# 검증 데이터에 대한 예측 수행\n",
    "dvalid = xgb.DMatrix(X_valid)\n",
    "\n",
    "y_pred = xgb_model.predict(dvalid)\n",
    "\n",
    "# MAE 계산\n",
    "mae = mean_absolute_error(y_valid, y_pred)\n",
    "print(\"MAE:\", mae)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# 현재 날짜와 시각 구하기\n",
    "now = datetime.datetime.now()\n",
    "date_list = [now.month, now.day, now.hour, now.minute , now.second]\n",
    "# 현재 날짜와 시각 출력하기\n",
    "print(\"현재 날짜 및 시각 : \", date_list)\n",
    "\n",
    "# 제출 파일 생성\n",
    "\n",
    "submission['yield'] = y_preds\n",
    "submission.to_csv(f'submission_KFOLD_XGB_MCA_{date_list[0]}_{date_list[1]}_{date_list[2]}_{date_list[3]}_{date_list[4]}.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}