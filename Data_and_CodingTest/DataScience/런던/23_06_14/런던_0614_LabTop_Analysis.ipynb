{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import platform\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "import seaborn as sns\n",
    "%precision 3\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "#그래프를 주피터 놋북에 그리기 위해\n",
    "import numpy as np\n",
    "import copy\n",
    "from matplotlib import ticker\n",
    "from scipy.stats import probplot\n",
    "from scipy import stats\n",
    "import collections\n",
    "import itertools\n",
    "import math\n",
    "#from varname import nameof\n",
    "import sys\n",
    "from scipy import stats\n",
    "from scipy.stats import bernoulli\n",
    "import scipy\n",
    "from collections import deque\n",
    "from sympy import Symbol, solve\n",
    "\n",
    "#히스토그램 그리기\n",
    "# Window\n",
    "if platform.system() == 'Windows':\n",
    "    matplotlib.rc('font', family='Malgun Gothic')\n",
    "elif platform.system() == 'Darwin': # Mac\n",
    "    matplotlib.rc('font', family='AppleGothic')\n",
    "else: #linux\n",
    "    matplotlib.rc('font', family='NanumGothic')\n",
    "\n",
    "# 그래프에 마이너스 표시가 되도록 변경\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 한글 폰트 설정\n",
    "font_location = 'C:/Windows/Fonts/MALGUNSL.TTF' #맑은고딕\n",
    "font_name = font_manager.FontProperties(fname=font_location).get_name()\n",
    "rc('font',family=font_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = '../data-science-london-scikit-learn/'\n",
    "\n",
    "train = pd.read_csv(data_path + 'train.csv' , header = None)\n",
    "test = pd.read_csv(data_path + 'test.csv' , header = None)\n",
    "trainLabels = pd.read_csv(data_path + 'trainLabels.csv' , header = None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "           0         1         2         3          4         5         6   \\\n0    0.299403 -1.226624  1.498425 -1.176150   5.289853  0.208297  2.404498   \n1   -1.174176  0.332157  0.949919 -1.285328   2.199061 -0.151268 -0.427039   \n2    1.192222 -0.414371  0.067054 -2.233568   3.658881  0.089007  0.203439   \n3    1.573270 -0.580318 -0.866332 -0.603812   3.125716  0.870321 -0.161992   \n4   -0.613071 -0.644204  1.112558 -0.032397   3.490142 -0.011935  1.443521   \n..        ...       ...       ...       ...        ...       ...       ...   \n995 -0.310429  0.826811 -0.952245  0.768850   1.877520  1.320646  1.944609   \n996 -1.853879  0.246726  0.459921 -2.074267   7.599220 -0.138355 -4.501900   \n997  0.912748 -1.734039 -1.047035  0.217573  13.457812  0.162771 -2.250521   \n998  2.439780 -0.735511 -0.902426  1.365036 -10.430299 -0.856859  2.686474   \n999  0.228994 -0.085453  0.876582  1.057401  -1.404015 -1.091965  0.639176   \n\n           7         8         9   ...        30        31        32  \\\n0    1.594506 -0.051608  0.663234  ... -0.850465 -0.622990 -1.833057   \n1    2.619246 -0.765884 -0.093780  ... -0.819750  0.012037  2.038836   \n2   -4.219054 -1.184919 -1.240310  ... -0.604501  0.750054 -3.360521   \n3    4.499666  1.038741 -1.092716  ...  1.022959  1.275598 -3.480110   \n4   -4.290282 -1.761308  0.807652  ...  0.513906 -1.803473  0.518579   \n..        ...       ...       ...  ...       ...       ...       ...   \n995  1.191420 -0.127724  0.070937  ... -0.600411 -0.383792  0.745596   \n996  0.630634 -1.590533 -1.112949  ...  0.361736  0.240052 -0.856196   \n997  2.216161 -0.378326  0.642114  ...  1.195896 -1.073806 -2.754369   \n998  0.292035  0.585388 -0.876965  ...  2.262326 -0.039488  0.773876   \n999  0.701332 -0.906577 -0.390940  ...  0.471415  1.024757 -1.796571   \n\n           33        34        35        36        37        38        39  \n0    0.293024  3.552681  0.717611  3.305972 -2.715559 -2.682409  0.101050  \n1    0.468579 -0.517657  0.422326  0.803699  1.213219  1.382932 -1.817761  \n2    0.856988 -2.751451 -1.582735  1.672246  0.656438 -0.932473  2.987436  \n3   -1.065252  2.153133  1.563539  2.767117  0.215748  0.619645  1.883397  \n4   -0.205029 -4.744566 -1.520015  1.830651  0.870772 -1.894609  0.408332  \n..        ...       ...       ...       ...       ...       ...       ...  \n995 -0.698598 -2.729937 -0.431535  0.372873  1.019092 -2.672811 -0.295141  \n996 -0.072481 -2.935896  0.582411 -2.613407  0.036687  2.809310  4.412567  \n997  1.814864 -4.190105 -1.116441 -2.100125  0.061513  0.895536  0.813686  \n998 -0.916066  2.604827 -0.649874 -3.423674  0.229748 -2.311088 -3.422217  \n999  0.603161  0.862705  0.747234  3.275681  0.400372 -3.431031  2.370080  \n\n[1000 rows x 40 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n      <th>36</th>\n      <th>37</th>\n      <th>38</th>\n      <th>39</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.299403</td>\n      <td>-1.226624</td>\n      <td>1.498425</td>\n      <td>-1.176150</td>\n      <td>5.289853</td>\n      <td>0.208297</td>\n      <td>2.404498</td>\n      <td>1.594506</td>\n      <td>-0.051608</td>\n      <td>0.663234</td>\n      <td>...</td>\n      <td>-0.850465</td>\n      <td>-0.622990</td>\n      <td>-1.833057</td>\n      <td>0.293024</td>\n      <td>3.552681</td>\n      <td>0.717611</td>\n      <td>3.305972</td>\n      <td>-2.715559</td>\n      <td>-2.682409</td>\n      <td>0.101050</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.174176</td>\n      <td>0.332157</td>\n      <td>0.949919</td>\n      <td>-1.285328</td>\n      <td>2.199061</td>\n      <td>-0.151268</td>\n      <td>-0.427039</td>\n      <td>2.619246</td>\n      <td>-0.765884</td>\n      <td>-0.093780</td>\n      <td>...</td>\n      <td>-0.819750</td>\n      <td>0.012037</td>\n      <td>2.038836</td>\n      <td>0.468579</td>\n      <td>-0.517657</td>\n      <td>0.422326</td>\n      <td>0.803699</td>\n      <td>1.213219</td>\n      <td>1.382932</td>\n      <td>-1.817761</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.192222</td>\n      <td>-0.414371</td>\n      <td>0.067054</td>\n      <td>-2.233568</td>\n      <td>3.658881</td>\n      <td>0.089007</td>\n      <td>0.203439</td>\n      <td>-4.219054</td>\n      <td>-1.184919</td>\n      <td>-1.240310</td>\n      <td>...</td>\n      <td>-0.604501</td>\n      <td>0.750054</td>\n      <td>-3.360521</td>\n      <td>0.856988</td>\n      <td>-2.751451</td>\n      <td>-1.582735</td>\n      <td>1.672246</td>\n      <td>0.656438</td>\n      <td>-0.932473</td>\n      <td>2.987436</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.573270</td>\n      <td>-0.580318</td>\n      <td>-0.866332</td>\n      <td>-0.603812</td>\n      <td>3.125716</td>\n      <td>0.870321</td>\n      <td>-0.161992</td>\n      <td>4.499666</td>\n      <td>1.038741</td>\n      <td>-1.092716</td>\n      <td>...</td>\n      <td>1.022959</td>\n      <td>1.275598</td>\n      <td>-3.480110</td>\n      <td>-1.065252</td>\n      <td>2.153133</td>\n      <td>1.563539</td>\n      <td>2.767117</td>\n      <td>0.215748</td>\n      <td>0.619645</td>\n      <td>1.883397</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.613071</td>\n      <td>-0.644204</td>\n      <td>1.112558</td>\n      <td>-0.032397</td>\n      <td>3.490142</td>\n      <td>-0.011935</td>\n      <td>1.443521</td>\n      <td>-4.290282</td>\n      <td>-1.761308</td>\n      <td>0.807652</td>\n      <td>...</td>\n      <td>0.513906</td>\n      <td>-1.803473</td>\n      <td>0.518579</td>\n      <td>-0.205029</td>\n      <td>-4.744566</td>\n      <td>-1.520015</td>\n      <td>1.830651</td>\n      <td>0.870772</td>\n      <td>-1.894609</td>\n      <td>0.408332</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>-0.310429</td>\n      <td>0.826811</td>\n      <td>-0.952245</td>\n      <td>0.768850</td>\n      <td>1.877520</td>\n      <td>1.320646</td>\n      <td>1.944609</td>\n      <td>1.191420</td>\n      <td>-0.127724</td>\n      <td>0.070937</td>\n      <td>...</td>\n      <td>-0.600411</td>\n      <td>-0.383792</td>\n      <td>0.745596</td>\n      <td>-0.698598</td>\n      <td>-2.729937</td>\n      <td>-0.431535</td>\n      <td>0.372873</td>\n      <td>1.019092</td>\n      <td>-2.672811</td>\n      <td>-0.295141</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>-1.853879</td>\n      <td>0.246726</td>\n      <td>0.459921</td>\n      <td>-2.074267</td>\n      <td>7.599220</td>\n      <td>-0.138355</td>\n      <td>-4.501900</td>\n      <td>0.630634</td>\n      <td>-1.590533</td>\n      <td>-1.112949</td>\n      <td>...</td>\n      <td>0.361736</td>\n      <td>0.240052</td>\n      <td>-0.856196</td>\n      <td>-0.072481</td>\n      <td>-2.935896</td>\n      <td>0.582411</td>\n      <td>-2.613407</td>\n      <td>0.036687</td>\n      <td>2.809310</td>\n      <td>4.412567</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>0.912748</td>\n      <td>-1.734039</td>\n      <td>-1.047035</td>\n      <td>0.217573</td>\n      <td>13.457812</td>\n      <td>0.162771</td>\n      <td>-2.250521</td>\n      <td>2.216161</td>\n      <td>-0.378326</td>\n      <td>0.642114</td>\n      <td>...</td>\n      <td>1.195896</td>\n      <td>-1.073806</td>\n      <td>-2.754369</td>\n      <td>1.814864</td>\n      <td>-4.190105</td>\n      <td>-1.116441</td>\n      <td>-2.100125</td>\n      <td>0.061513</td>\n      <td>0.895536</td>\n      <td>0.813686</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>2.439780</td>\n      <td>-0.735511</td>\n      <td>-0.902426</td>\n      <td>1.365036</td>\n      <td>-10.430299</td>\n      <td>-0.856859</td>\n      <td>2.686474</td>\n      <td>0.292035</td>\n      <td>0.585388</td>\n      <td>-0.876965</td>\n      <td>...</td>\n      <td>2.262326</td>\n      <td>-0.039488</td>\n      <td>0.773876</td>\n      <td>-0.916066</td>\n      <td>2.604827</td>\n      <td>-0.649874</td>\n      <td>-3.423674</td>\n      <td>0.229748</td>\n      <td>-2.311088</td>\n      <td>-3.422217</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>0.228994</td>\n      <td>-0.085453</td>\n      <td>0.876582</td>\n      <td>1.057401</td>\n      <td>-1.404015</td>\n      <td>-1.091965</td>\n      <td>0.639176</td>\n      <td>0.701332</td>\n      <td>-0.906577</td>\n      <td>-0.390940</td>\n      <td>...</td>\n      <td>0.471415</td>\n      <td>1.024757</td>\n      <td>-1.796571</td>\n      <td>0.603161</td>\n      <td>0.862705</td>\n      <td>0.747234</td>\n      <td>3.275681</td>\n      <td>0.400372</td>\n      <td>-3.431031</td>\n      <td>2.370080</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 40 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "            0         1         2         3         4         5         6   \\\n0     2.808909 -0.242894 -0.546421  0.255162  1.749736 -0.030458 -1.322071   \n1    -0.374101  0.537669  0.081063  0.756773  0.915231  2.557282  3.703187   \n2    -0.088370  0.154743  0.380716 -1.176126  1.699867 -0.258627 -1.384999   \n3    -0.685635  0.501283  1.873375  0.215224 -3.983468 -0.103637  4.136113   \n4     0.350867  0.721897 -0.477104 -1.748776 -2.627405  1.075433  4.954253   \n...        ...       ...       ...       ...       ...       ...       ...   \n8995  0.171644 -0.806952 -2.045671  0.021156  2.258491  0.429469  0.857187   \n8996  1.168564 -0.911253  1.685492  0.867183  3.606170 -0.673875 -1.889365   \n8997  0.052274 -1.736558 -0.263699 -0.219329  8.918393 -1.258320 -3.361146   \n8998  1.443659  0.651892  0.550724 -1.146664  2.621641 -0.867143  0.312742   \n8999 -0.429145 -0.110821  1.257230 -0.088150 -3.925020 -0.251062 -2.212032   \n\n            7         8         9   ...        30        31        32  \\\n0     3.578071 -0.667578 -0.884257  ... -0.261688 -0.224375 -1.675606   \n1     1.673835 -0.764122 -1.228040  ... -0.969463  0.574154 -2.200519   \n2     1.093584  1.596633  0.230631  ... -0.769885 -0.005143  1.467490   \n3    -0.225431 -1.515015 -1.071763  ...  0.968609  2.386412 -0.131219   \n4    -3.293501 -0.760369  0.204360  ...  0.260553 -2.045650 -2.173227   \n...        ...       ...       ...  ...       ...       ...       ...   \n8995  0.972600  1.707492  1.676370  ... -1.366312  0.276543 -0.732764   \n8996  0.411385 -0.206817 -0.705771  ...  0.557757  0.379841 -1.474198   \n8997  0.893366 -0.631669  1.887286  ...  2.117847 -1.050824  0.182872   \n8998  1.078004 -1.212524 -0.028143  ...  0.631480  1.186236 -1.098508   \n8999 -2.692121 -0.005468  0.186300  ...  0.384256  0.520610  1.594873   \n\n            33        34        35        36        37        38        39  \n0    -0.479584 -0.244388 -0.672355  0.517860  0.010665 -0.419214  2.818387  \n1    -1.612240  0.179031 -2.924596  0.643610 -1.470939 -0.067408 -0.976265  \n2     0.483803 -3.542981  0.814561 -1.652948  1.265866 -1.749248  1.773784  \n3     0.285646  2.302069  1.255588 -1.563090 -0.125258 -1.030761 -2.945329  \n4     0.372992  0.450700 -0.211657  1.301359 -0.522164  2.484883  0.039213  \n...        ...       ...       ...       ...       ...       ...       ...  \n8995  0.243930 -1.151233 -0.274298  0.573013  1.109814 -1.905965  1.457601  \n8996 -0.322943  1.964519  0.122384  0.678023  2.024129  0.386542  1.104493  \n8997  0.242725  0.670161  0.112752 -3.006949  1.179606  1.156340 -1.218561  \n8998  1.159658 -1.957241  0.482533  3.777669 -0.424954  1.333374  2.325271  \n8999 -0.352201  0.152333 -1.772249  0.758809  0.617485  1.464438  3.301445  \n\n[9000 rows x 40 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n      <th>36</th>\n      <th>37</th>\n      <th>38</th>\n      <th>39</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.808909</td>\n      <td>-0.242894</td>\n      <td>-0.546421</td>\n      <td>0.255162</td>\n      <td>1.749736</td>\n      <td>-0.030458</td>\n      <td>-1.322071</td>\n      <td>3.578071</td>\n      <td>-0.667578</td>\n      <td>-0.884257</td>\n      <td>...</td>\n      <td>-0.261688</td>\n      <td>-0.224375</td>\n      <td>-1.675606</td>\n      <td>-0.479584</td>\n      <td>-0.244388</td>\n      <td>-0.672355</td>\n      <td>0.517860</td>\n      <td>0.010665</td>\n      <td>-0.419214</td>\n      <td>2.818387</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.374101</td>\n      <td>0.537669</td>\n      <td>0.081063</td>\n      <td>0.756773</td>\n      <td>0.915231</td>\n      <td>2.557282</td>\n      <td>3.703187</td>\n      <td>1.673835</td>\n      <td>-0.764122</td>\n      <td>-1.228040</td>\n      <td>...</td>\n      <td>-0.969463</td>\n      <td>0.574154</td>\n      <td>-2.200519</td>\n      <td>-1.612240</td>\n      <td>0.179031</td>\n      <td>-2.924596</td>\n      <td>0.643610</td>\n      <td>-1.470939</td>\n      <td>-0.067408</td>\n      <td>-0.976265</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.088370</td>\n      <td>0.154743</td>\n      <td>0.380716</td>\n      <td>-1.176126</td>\n      <td>1.699867</td>\n      <td>-0.258627</td>\n      <td>-1.384999</td>\n      <td>1.093584</td>\n      <td>1.596633</td>\n      <td>0.230631</td>\n      <td>...</td>\n      <td>-0.769885</td>\n      <td>-0.005143</td>\n      <td>1.467490</td>\n      <td>0.483803</td>\n      <td>-3.542981</td>\n      <td>0.814561</td>\n      <td>-1.652948</td>\n      <td>1.265866</td>\n      <td>-1.749248</td>\n      <td>1.773784</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.685635</td>\n      <td>0.501283</td>\n      <td>1.873375</td>\n      <td>0.215224</td>\n      <td>-3.983468</td>\n      <td>-0.103637</td>\n      <td>4.136113</td>\n      <td>-0.225431</td>\n      <td>-1.515015</td>\n      <td>-1.071763</td>\n      <td>...</td>\n      <td>0.968609</td>\n      <td>2.386412</td>\n      <td>-0.131219</td>\n      <td>0.285646</td>\n      <td>2.302069</td>\n      <td>1.255588</td>\n      <td>-1.563090</td>\n      <td>-0.125258</td>\n      <td>-1.030761</td>\n      <td>-2.945329</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.350867</td>\n      <td>0.721897</td>\n      <td>-0.477104</td>\n      <td>-1.748776</td>\n      <td>-2.627405</td>\n      <td>1.075433</td>\n      <td>4.954253</td>\n      <td>-3.293501</td>\n      <td>-0.760369</td>\n      <td>0.204360</td>\n      <td>...</td>\n      <td>0.260553</td>\n      <td>-2.045650</td>\n      <td>-2.173227</td>\n      <td>0.372992</td>\n      <td>0.450700</td>\n      <td>-0.211657</td>\n      <td>1.301359</td>\n      <td>-0.522164</td>\n      <td>2.484883</td>\n      <td>0.039213</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8995</th>\n      <td>0.171644</td>\n      <td>-0.806952</td>\n      <td>-2.045671</td>\n      <td>0.021156</td>\n      <td>2.258491</td>\n      <td>0.429469</td>\n      <td>0.857187</td>\n      <td>0.972600</td>\n      <td>1.707492</td>\n      <td>1.676370</td>\n      <td>...</td>\n      <td>-1.366312</td>\n      <td>0.276543</td>\n      <td>-0.732764</td>\n      <td>0.243930</td>\n      <td>-1.151233</td>\n      <td>-0.274298</td>\n      <td>0.573013</td>\n      <td>1.109814</td>\n      <td>-1.905965</td>\n      <td>1.457601</td>\n    </tr>\n    <tr>\n      <th>8996</th>\n      <td>1.168564</td>\n      <td>-0.911253</td>\n      <td>1.685492</td>\n      <td>0.867183</td>\n      <td>3.606170</td>\n      <td>-0.673875</td>\n      <td>-1.889365</td>\n      <td>0.411385</td>\n      <td>-0.206817</td>\n      <td>-0.705771</td>\n      <td>...</td>\n      <td>0.557757</td>\n      <td>0.379841</td>\n      <td>-1.474198</td>\n      <td>-0.322943</td>\n      <td>1.964519</td>\n      <td>0.122384</td>\n      <td>0.678023</td>\n      <td>2.024129</td>\n      <td>0.386542</td>\n      <td>1.104493</td>\n    </tr>\n    <tr>\n      <th>8997</th>\n      <td>0.052274</td>\n      <td>-1.736558</td>\n      <td>-0.263699</td>\n      <td>-0.219329</td>\n      <td>8.918393</td>\n      <td>-1.258320</td>\n      <td>-3.361146</td>\n      <td>0.893366</td>\n      <td>-0.631669</td>\n      <td>1.887286</td>\n      <td>...</td>\n      <td>2.117847</td>\n      <td>-1.050824</td>\n      <td>0.182872</td>\n      <td>0.242725</td>\n      <td>0.670161</td>\n      <td>0.112752</td>\n      <td>-3.006949</td>\n      <td>1.179606</td>\n      <td>1.156340</td>\n      <td>-1.218561</td>\n    </tr>\n    <tr>\n      <th>8998</th>\n      <td>1.443659</td>\n      <td>0.651892</td>\n      <td>0.550724</td>\n      <td>-1.146664</td>\n      <td>2.621641</td>\n      <td>-0.867143</td>\n      <td>0.312742</td>\n      <td>1.078004</td>\n      <td>-1.212524</td>\n      <td>-0.028143</td>\n      <td>...</td>\n      <td>0.631480</td>\n      <td>1.186236</td>\n      <td>-1.098508</td>\n      <td>1.159658</td>\n      <td>-1.957241</td>\n      <td>0.482533</td>\n      <td>3.777669</td>\n      <td>-0.424954</td>\n      <td>1.333374</td>\n      <td>2.325271</td>\n    </tr>\n    <tr>\n      <th>8999</th>\n      <td>-0.429145</td>\n      <td>-0.110821</td>\n      <td>1.257230</td>\n      <td>-0.088150</td>\n      <td>-3.925020</td>\n      <td>-0.251062</td>\n      <td>-2.212032</td>\n      <td>-2.692121</td>\n      <td>-0.005468</td>\n      <td>0.186300</td>\n      <td>...</td>\n      <td>0.384256</td>\n      <td>0.520610</td>\n      <td>1.594873</td>\n      <td>-0.352201</td>\n      <td>0.152333</td>\n      <td>-1.772249</td>\n      <td>0.758809</td>\n      <td>0.617485</td>\n      <td>1.464438</td>\n      <td>3.301445</td>\n    </tr>\n  </tbody>\n</table>\n<p>9000 rows × 40 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "     predict\n0          1\n1          0\n2          0\n3          1\n4          0\n..       ...\n995        0\n996        1\n997        1\n998        0\n999        0\n\n[1000 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>predict</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLabels = trainLabels.rename(columns={0: 'predict'})\n",
    "trainLabels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 40 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       1000 non-null   float64\n",
      " 1   1       1000 non-null   float64\n",
      " 2   2       1000 non-null   float64\n",
      " 3   3       1000 non-null   float64\n",
      " 4   4       1000 non-null   float64\n",
      " 5   5       1000 non-null   float64\n",
      " 6   6       1000 non-null   float64\n",
      " 7   7       1000 non-null   float64\n",
      " 8   8       1000 non-null   float64\n",
      " 9   9       1000 non-null   float64\n",
      " 10  10      1000 non-null   float64\n",
      " 11  11      1000 non-null   float64\n",
      " 12  12      1000 non-null   float64\n",
      " 13  13      1000 non-null   float64\n",
      " 14  14      1000 non-null   float64\n",
      " 15  15      1000 non-null   float64\n",
      " 16  16      1000 non-null   float64\n",
      " 17  17      1000 non-null   float64\n",
      " 18  18      1000 non-null   float64\n",
      " 19  19      1000 non-null   float64\n",
      " 20  20      1000 non-null   float64\n",
      " 21  21      1000 non-null   float64\n",
      " 22  22      1000 non-null   float64\n",
      " 23  23      1000 non-null   float64\n",
      " 24  24      1000 non-null   float64\n",
      " 25  25      1000 non-null   float64\n",
      " 26  26      1000 non-null   float64\n",
      " 27  27      1000 non-null   float64\n",
      " 28  28      1000 non-null   float64\n",
      " 29  29      1000 non-null   float64\n",
      " 30  30      1000 non-null   float64\n",
      " 31  31      1000 non-null   float64\n",
      " 32  32      1000 non-null   float64\n",
      " 33  33      1000 non-null   float64\n",
      " 34  34      1000 non-null   float64\n",
      " 35  35      1000 non-null   float64\n",
      " 36  36      1000 non-null   float64\n",
      " 37  37      1000 non-null   float64\n",
      " 38  38      1000 non-null   float64\n",
      " 39  39      1000 non-null   float64\n",
      "dtypes: float64(40)\n",
      "memory usage: 312.6 KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                0            1            2            3            4   \\\ncount  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \nmean      0.025596    -0.024526    -0.024088    -0.002271     1.092329   \nstd       1.008282     1.016298     0.979109     0.970575     4.538834   \nmin      -3.365711    -3.492086    -2.695602    -3.460471   -16.421901   \n25%      -0.669010    -0.693937    -0.698830    -0.617557    -1.801997   \n50%       0.027895    -0.033194     0.008145     0.002327     0.862818   \n75%       0.762520     0.682753     0.661434     0.640743     3.843172   \nmax       3.326246     3.583870     2.546507     3.088738    17.565345   \n\n                5            6            7            8            9   ...  \\\ncount  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  ...   \nmean     -0.006250     0.497342    -0.037883     0.026391    -0.003597  ...   \nstd       0.989128     2.118819     2.232256     1.001064     1.013520  ...   \nmin      -3.041250    -7.224761    -6.509084    -3.145588    -2.749812  ...   \n25%      -0.732265    -0.838619    -1.604037    -0.677562    -0.682220  ...   \n50%       0.027041     0.582321     0.018809     0.022092    -0.036110  ...   \n75%       0.671456     1.913664     1.438304     0.741310     0.665364  ...   \nmax       3.102997     7.592666     7.130097     3.145258     3.919426  ...   \n\n                30           31           32           33           34  \\\ncount  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \nmean      0.030651     0.022951    -0.542491    -0.011608    -0.483507   \nstd       1.011645     1.001375     2.239939     1.022456     2.121281   \nmin      -3.379194    -2.971125    -7.840890    -2.999564    -7.124105   \n25%      -0.659457    -0.696032    -2.121943    -0.664550    -1.879247   \n50%       0.049416     0.049778    -0.568262    -0.028097    -0.493575   \n75%       0.747031     0.699917     0.939348     0.651374     1.005795   \nmax       2.844792     3.688047     7.160379     3.353631     6.005818   \n\n                35           36           37           38           39  \ncount  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \nmean      0.033371     0.567185     0.006849    -0.892659     0.609451  \nstd       1.007044     2.227876     0.997635     2.022022     2.045439  \nmin      -2.952358    -5.452254    -3.473913    -8.051722    -7.799086  \n25%      -0.642861    -1.059786    -0.691162    -2.220126    -0.565041  \n50%       0.037732     0.455474     0.038284    -0.855470     0.779944  \n75%       0.691800     2.122157     0.693535     0.388698     1.992193  \nmax       3.420561     6.603499     3.492548     5.774120     6.803984  \n\n[8 rows x 40 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n      <th>36</th>\n      <th>37</th>\n      <th>38</th>\n      <th>39</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>...</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.025596</td>\n      <td>-0.024526</td>\n      <td>-0.024088</td>\n      <td>-0.002271</td>\n      <td>1.092329</td>\n      <td>-0.006250</td>\n      <td>0.497342</td>\n      <td>-0.037883</td>\n      <td>0.026391</td>\n      <td>-0.003597</td>\n      <td>...</td>\n      <td>0.030651</td>\n      <td>0.022951</td>\n      <td>-0.542491</td>\n      <td>-0.011608</td>\n      <td>-0.483507</td>\n      <td>0.033371</td>\n      <td>0.567185</td>\n      <td>0.006849</td>\n      <td>-0.892659</td>\n      <td>0.609451</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.008282</td>\n      <td>1.016298</td>\n      <td>0.979109</td>\n      <td>0.970575</td>\n      <td>4.538834</td>\n      <td>0.989128</td>\n      <td>2.118819</td>\n      <td>2.232256</td>\n      <td>1.001064</td>\n      <td>1.013520</td>\n      <td>...</td>\n      <td>1.011645</td>\n      <td>1.001375</td>\n      <td>2.239939</td>\n      <td>1.022456</td>\n      <td>2.121281</td>\n      <td>1.007044</td>\n      <td>2.227876</td>\n      <td>0.997635</td>\n      <td>2.022022</td>\n      <td>2.045439</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-3.365711</td>\n      <td>-3.492086</td>\n      <td>-2.695602</td>\n      <td>-3.460471</td>\n      <td>-16.421901</td>\n      <td>-3.041250</td>\n      <td>-7.224761</td>\n      <td>-6.509084</td>\n      <td>-3.145588</td>\n      <td>-2.749812</td>\n      <td>...</td>\n      <td>-3.379194</td>\n      <td>-2.971125</td>\n      <td>-7.840890</td>\n      <td>-2.999564</td>\n      <td>-7.124105</td>\n      <td>-2.952358</td>\n      <td>-5.452254</td>\n      <td>-3.473913</td>\n      <td>-8.051722</td>\n      <td>-7.799086</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-0.669010</td>\n      <td>-0.693937</td>\n      <td>-0.698830</td>\n      <td>-0.617557</td>\n      <td>-1.801997</td>\n      <td>-0.732265</td>\n      <td>-0.838619</td>\n      <td>-1.604037</td>\n      <td>-0.677562</td>\n      <td>-0.682220</td>\n      <td>...</td>\n      <td>-0.659457</td>\n      <td>-0.696032</td>\n      <td>-2.121943</td>\n      <td>-0.664550</td>\n      <td>-1.879247</td>\n      <td>-0.642861</td>\n      <td>-1.059786</td>\n      <td>-0.691162</td>\n      <td>-2.220126</td>\n      <td>-0.565041</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.027895</td>\n      <td>-0.033194</td>\n      <td>0.008145</td>\n      <td>0.002327</td>\n      <td>0.862818</td>\n      <td>0.027041</td>\n      <td>0.582321</td>\n      <td>0.018809</td>\n      <td>0.022092</td>\n      <td>-0.036110</td>\n      <td>...</td>\n      <td>0.049416</td>\n      <td>0.049778</td>\n      <td>-0.568262</td>\n      <td>-0.028097</td>\n      <td>-0.493575</td>\n      <td>0.037732</td>\n      <td>0.455474</td>\n      <td>0.038284</td>\n      <td>-0.855470</td>\n      <td>0.779944</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.762520</td>\n      <td>0.682753</td>\n      <td>0.661434</td>\n      <td>0.640743</td>\n      <td>3.843172</td>\n      <td>0.671456</td>\n      <td>1.913664</td>\n      <td>1.438304</td>\n      <td>0.741310</td>\n      <td>0.665364</td>\n      <td>...</td>\n      <td>0.747031</td>\n      <td>0.699917</td>\n      <td>0.939348</td>\n      <td>0.651374</td>\n      <td>1.005795</td>\n      <td>0.691800</td>\n      <td>2.122157</td>\n      <td>0.693535</td>\n      <td>0.388698</td>\n      <td>1.992193</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>3.326246</td>\n      <td>3.583870</td>\n      <td>2.546507</td>\n      <td>3.088738</td>\n      <td>17.565345</td>\n      <td>3.102997</td>\n      <td>7.592666</td>\n      <td>7.130097</td>\n      <td>3.145258</td>\n      <td>3.919426</td>\n      <td>...</td>\n      <td>2.844792</td>\n      <td>3.688047</td>\n      <td>7.160379</td>\n      <td>3.353631</td>\n      <td>6.005818</td>\n      <td>3.420561</td>\n      <td>6.603499</td>\n      <td>3.492548</td>\n      <td>5.774120</td>\n      <td>6.803984</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 40 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBclassifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "# Train 데이터의 인덱스를 타깃 변수로 사용하여 이진 분류 수행\n",
    "X = train\n",
    "y = trainLabels\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# 이진 분류 모델 생성 및 학습\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 모델 평가\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "      Solution\nId            \n1            1\n2            0\n3            0\n4            0\n5            0\n...        ...\n8996         1\n8997         1\n8998         1\n8999         0\n9000         1\n\n[9000 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Solution</th>\n    </tr>\n    <tr>\n      <th>Id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8996</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8997</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8998</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8999</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9000</th>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>9000 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = model.predict(test)\n",
    "\n",
    "# 기존의 df_preds 데이터프레임\n",
    "# df_preds = pd.DataFrame(y_preds)\n",
    "\n",
    "df_preds = pd.DataFrame(y_preds)\n",
    "df_preds.columns = ['Solution']\n",
    "df_preds.index = df_preds.index + 1\n",
    "df_preds.index.name = 'Id'\n",
    "\n",
    "df_preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# import datetime\n",
    "#\n",
    "# # 현재 날짜와 시각 구하기\n",
    "# now = datetime.datetime.now()\n",
    "# date_list = [now.month, now.day, now.hour, now.minute, now.second]\n",
    "# # 현재 날짜와 시각 출력하기\n",
    "# print(\"현재 날짜 및 시각 : \", date_list)\n",
    "#\n",
    "# # 제출 파일 생성\n",
    "#\n",
    "# # submission['Survived'] = y_preds\n",
    "# df_preds.to_csv(f'XGBClassifier_{date_list[0]}_{date_list[1]}_{date_list[2]}_{date_list[3]}_{date_list[4]}.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 베이지안 활용한 XGBoostRegreesion 돌려보기"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "def mae_scorer(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'Mae' , mean_absolute_error(labels, preds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def accuracy_scorer(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    preds_binary = (preds > 0.5).astype(int)\n",
    "    return 'accuracy', accuracy_score(labels, preds_binary)\n",
    "#\n",
    "# def accuracy_scorer(preds, dtrain):\n",
    "#     y_true = dtrain.get_label()\n",
    "#     y_pred = (preds > 0.5).astype(int)\n",
    "#     accuracy = accuracy_score(y_true, y_pred)\n",
    "#     return accuracy\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "#\n",
    "X = sparse.csr_matrix(train)\n",
    "X_test = sparse.csr_matrix(test)\n",
    "\n",
    "\n",
    "y = trainLabels['predict'].values\n",
    "\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 분리\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# 베이지안 최적화용 데이터셋\n",
    "\n",
    "bayes_dtrain = xgb.DMatrix(X_train , y_train)\n",
    "bayes_dvalid = xgb.DMatrix(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "# 베이지안 최적화를 위한 하이퍼파라미터 범위\n",
    "param_bounds = {'max_depth' : (4 , 10) , # 개별 트리의 최대 깊이, 트리 깊이가 깊을수록 모델이 복잡해지고 과대적합 우려\n",
    "                # 값이 클수록 깊이가 한 단계만 늘어나도 메모리 사용량이 급격히 많아진다.\n",
    "                # 일반적으로 3~10 사이의 값을 주로 사용한다.\n",
    "\n",
    "                'subsample' : (0.5 , 1), # 개별 트리를 훈련할 때 사용할 데이터 샘플링 비율\n",
    "                # 0~1 사이 값으로 설정할 수 있다.\n",
    "                # 0.5 로 설정하면 전체 데이터의 50%를 사용해 트리를 생성\n",
    "\n",
    "                'colsample_bytree' : (0.7 , 1.0), # 개별 트리를 훈련할 때 사용하는 피처 샘플링 비율\n",
    "                # subsample 과 유사한 개념, subsample은 전체 데이터에서 얼마나 샘플링할지 나타내는 비율\n",
    "                # colsample_bytree는 전체 피처에서 얼마나 샘플링할지 나타내는 비율\n",
    "                # 값이 작을수록 과대적합 방지 효과\n",
    "\n",
    "                'min_child_weight' : (5 , 10), # 과대적합 방지위한 값, 값이 클수록 과대적합 방지 효과가 있다.\n",
    "                'gamma' : (8 , 11), # 말단 노드가 분할하기 위한 최소 손실 감소 값\n",
    "                # 소실 감소가 gamma보다 크면 말단 노드를 분할\n",
    "                # 값이 클수록 과대적합 방지 효과가 있다.\n",
    "\n",
    "                'reg_alpha' : (7 , 9) , # L1 규제 조정 값 , 값이 클수록 과대적합 방지 효과\n",
    "                'reg_lambda' : (1.1 , 1.5), # L2 규제 조정값 , 값이 클수록 과대적합 방지 효과\n",
    "                'scale_pos_weight' : (1.4 , 1.6), # 뷸균형 데이터 가중치 조정 값 ,\n",
    "                # 타깃값이 불균형할 때 양성 값에 scale_pos_weight 만큼 가중치를 줘서 균형을 맞춤(타깃값 1을 양성 값으로 간주)\n",
    "                # 일반적으로 scale_pos_weight 값을 (음성 타깃값 개수 / 양성 타깃값 개수) 로 설정\n",
    "                'learning_rate' : (0.02, 1)} # 학습률( 부스팅 스텝을 반복하면서 모델을 업데이트하는 데 사용되는 비율)\n",
    "\n",
    "\n",
    "# 값이 고정된 하이퍼파라미터\n",
    "\n",
    "fixed_params = {'objective' : 'binary:logistic' ,# 훈련 목적 , binary : logistic( 확률값을 구하는 이진분류)\n",
    "\n",
    "                # reg : squarederror (회귀 문제)\n",
    "                # 소프트맥스 함수를 사용하는 다중분류에서는 multi : softmax 사용\n",
    "                # 확률값을 구하는 다중분류에서는 'multi : softprob' 사용\n",
    "\n",
    "                'random_state' : 1991} # 랜덤 시드값(코드를 반복 실행해도 같은 결과가 나오게 지정하는 값)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def eval_function(max_depth, learning_rate, subsample, colsample_bytree, min_child_weight, reg_alpha, gamma, reg_lambda, scale_pos_weight):\n",
    "    params = {'max_depth': int(round(max_depth)),\n",
    "              'learning_rate': learning_rate,\n",
    "              'subsample': subsample,\n",
    "              'colsample_bytree': colsample_bytree,\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'gamma': gamma,\n",
    "              'reg_alpha': reg_alpha,\n",
    "              'reg_lambda': reg_lambda,\n",
    "              'scale_pos_weight': scale_pos_weight}\n",
    "\n",
    "    params.update(fixed_params)\n",
    "\n",
    "    print('하이퍼파라미터:', params)\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],\n",
    "              early_stopping_rounds=200, verbose=False)\n",
    "\n",
    "    preds = model.predict_proba(X_valid)[:, 1]\n",
    "    # 정확도 계산\n",
    "    accuracy = accuracy_score(y_valid, preds.round())\n",
    "    print(f'Accuracy: {accuracy}\\n')\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# 이하 생략\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |   gamma   | learni... | max_depth | min_ch... | reg_alpha | reg_la... | scale_... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "하이퍼파라미터: {'max_depth': 7, 'learning_rate': 0.610708108550211, 'subsample': 0.9818313802505146, 'colsample_bytree': 0.8646440511781974, 'min_child_weight': 7.118273996694524, 'gamma': 10.14556809911726, 'reg_alpha': 8.291788226133312, 'reg_lambda': 1.275034884505077, 'scale_pos_weight': 1.578354600156416, 'objective': 'binary:logistic', 'random_state': 1991}\n",
      "Accuracy: 0.855\n",
      "\n",
      "| \u001B[0m1        \u001B[0m | \u001B[0m0.855    \u001B[0m | \u001B[0m0.8646   \u001B[0m | \u001B[0m10.15    \u001B[0m | \u001B[0m0.6107   \u001B[0m | \u001B[0m7.269    \u001B[0m | \u001B[0m7.118    \u001B[0m | \u001B[0m8.292    \u001B[0m | \u001B[0m1.275    \u001B[0m | \u001B[0m1.578    \u001B[0m | \u001B[0m0.9818   \u001B[0m |\n",
      "하이퍼파라미터: {'max_depth': 7, 'learning_rate': 0.5383170213578464, 'subsample': 0.916309922773969, 'colsample_bytree': 0.8150324556477333, 'min_child_weight': 9.627983191463304, 'gamma': 10.375175114247995, 'reg_alpha': 7.142072116395774, 'reg_lambda': 1.1348517198806163, 'scale_pos_weight': 1.404043679488065, 'objective': 'binary:logistic', 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\park sung hyuk95\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\park sung hyuk95\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84\n",
      "\n",
      "| \u001B[0m2        \u001B[0m | \u001B[0m0.84     \u001B[0m | \u001B[0m0.815    \u001B[0m | \u001B[0m10.38    \u001B[0m | \u001B[0m0.5383   \u001B[0m | \u001B[0m7.408    \u001B[0m | \u001B[0m9.628    \u001B[0m | \u001B[0m7.142    \u001B[0m | \u001B[0m1.135    \u001B[0m | \u001B[0m1.404    \u001B[0m | \u001B[0m0.9163   \u001B[0m |\n",
      "하이퍼파라미터: {'max_depth': 9, 'learning_rate': 0.9790459753881088, 'subsample': 0.5716766437045232, 'colsample_bytree': 0.9334470252849552, 'min_child_weight': 7.30739681126466, 'gamma': 10.610036444740457, 'reg_alpha': 8.56105835257291, 'reg_lambda': 1.1473097703475734, 'scale_pos_weight': 1.5279842042655047, 'objective': 'binary:logistic', 'random_state': 1991}\n",
      "Accuracy: 0.82\n",
      "\n",
      "| \u001B[0m3        \u001B[0m | \u001B[0m0.82     \u001B[0m | \u001B[0m0.9334   \u001B[0m | \u001B[0m10.61    \u001B[0m | \u001B[0m0.979    \u001B[0m | \u001B[0m8.795    \u001B[0m | \u001B[0m7.307    \u001B[0m | \u001B[0m8.561    \u001B[0m | \u001B[0m1.147    \u001B[0m | \u001B[0m1.528    \u001B[0m | \u001B[0m0.5717   \u001B[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\park sung hyuk95\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하이퍼파라미터: {'max_depth': 9, 'learning_rate': 0.9363677223334654, 'subsample': 0.9154444873061973, 'colsample_bytree': 0.7237366663815606, 'min_child_weight': 9.457964776710647, 'gamma': 8.254805197743558, 'reg_alpha': 8.3182857346322, 'reg_lambda': 1.1875047687160731, 'scale_pos_weight': 1.563558309161644, 'objective': 'binary:logistic', 'random_state': 1991}\n",
      "Accuracy: 0.835\n",
      "\n",
      "| \u001B[0m4        \u001B[0m | \u001B[0m0.835    \u001B[0m | \u001B[0m0.7237   \u001B[0m | \u001B[0m8.255    \u001B[0m | \u001B[0m0.9364   \u001B[0m | \u001B[0m8.864    \u001B[0m | \u001B[0m9.458    \u001B[0m | \u001B[0m8.318    \u001B[0m | \u001B[0m1.188    \u001B[0m | \u001B[0m1.564    \u001B[0m | \u001B[0m0.9154   \u001B[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\park sung hyuk95\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하이퍼파라미터: {'max_depth': 5, 'learning_rate': 0.554153861380222, 'subsample': 0.8029198591955162, 'colsample_bytree': 0.7793853153298054, 'min_child_weight': 5.356563294958047, 'gamma': 10.56716613653328, 'reg_alpha': 8.6829996041405, 'reg_lambda': 1.4388915200917118, 'scale_pos_weight': 1.4118213211529071, 'objective': 'binary:logistic', 'random_state': 1991}\n",
      "Accuracy: 0.83\n",
      "\n",
      "| \u001B[0m5        \u001B[0m | \u001B[0m0.83     \u001B[0m | \u001B[0m0.7794   \u001B[0m | \u001B[0m10.57    \u001B[0m | \u001B[0m0.5542   \u001B[0m | \u001B[0m5.106    \u001B[0m | \u001B[0m5.357    \u001B[0m | \u001B[0m8.683    \u001B[0m | \u001B[0m1.439    \u001B[0m | \u001B[0m1.412    \u001B[0m | \u001B[0m0.8029   \u001B[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\park sung hyuk95\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하이퍼파라미터: {'max_depth': 7, 'learning_rate': 0.4051315191036389, 'subsample': 0.943125623844969, 'colsample_bytree': 0.8456010772784257, 'min_child_weight': 8.504928951267159, 'gamma': 9.894716615495389, 'reg_alpha': 7.550107924904835, 'reg_lambda': 1.3936277238149792, 'scale_pos_weight': 1.472024787282957, 'objective': 'binary:logistic', 'random_state': 1991}\n",
      "Accuracy: 0.855\n",
      "\n",
      "| \u001B[0m6        \u001B[0m | \u001B[0m0.855    \u001B[0m | \u001B[0m0.8456   \u001B[0m | \u001B[0m9.895    \u001B[0m | \u001B[0m0.4051   \u001B[0m | \u001B[0m7.214    \u001B[0m | \u001B[0m8.505    \u001B[0m | \u001B[0m7.55     \u001B[0m | \u001B[0m1.394    \u001B[0m | \u001B[0m1.472    \u001B[0m | \u001B[0m0.9431   \u001B[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\park sung hyuk95\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하이퍼파라미터: {'max_depth': 7, 'learning_rate': 0.5004763157163529, 'subsample': 0.9713101804966424, 'colsample_bytree': 0.8537064629232646, 'min_child_weight': 7.805044598311992, 'gamma': 10.010985777430271, 'reg_alpha': 7.916538851805258, 'reg_lambda': 1.3367601249053507, 'scale_pos_weight': 1.5264594201554176, 'objective': 'binary:logistic', 'random_state': 1991}\n",
      "Accuracy: 0.855\n",
      "\n",
      "| \u001B[0m7        \u001B[0m | \u001B[0m0.855    \u001B[0m | \u001B[0m0.8537   \u001B[0m | \u001B[0m10.01    \u001B[0m | \u001B[0m0.5005   \u001B[0m | \u001B[0m7.209    \u001B[0m | \u001B[0m7.805    \u001B[0m | \u001B[0m7.917    \u001B[0m | \u001B[0m1.337    \u001B[0m | \u001B[0m1.526    \u001B[0m | \u001B[0m0.9713   \u001B[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\park sung hyuk95\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하이퍼파라미터: {'max_depth': 7, 'learning_rate': 0.0352950144665526, 'subsample': 1.0, 'colsample_bytree': 0.8875613353243899, 'min_child_weight': 7.338307082266653, 'gamma': 9.273511076403677, 'reg_alpha': 7.803616969913589, 'reg_lambda': 1.456446019990264, 'scale_pos_weight': 1.6, 'objective': 'binary:logistic', 'random_state': 1991}\n",
      "Accuracy: 0.83\n",
      "\n",
      "| \u001B[0m8        \u001B[0m | \u001B[0m0.83     \u001B[0m | \u001B[0m0.8876   \u001B[0m | \u001B[0m9.274    \u001B[0m | \u001B[0m0.0353   \u001B[0m | \u001B[0m6.61     \u001B[0m | \u001B[0m7.338    \u001B[0m | \u001B[0m7.804    \u001B[0m | \u001B[0m1.456    \u001B[0m | \u001B[0m1.6      \u001B[0m | \u001B[0m1.0      \u001B[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\park sung hyuk95\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하이퍼파라미터: {'max_depth': 7, 'learning_rate': 0.8361704434026666, 'subsample': 1.0, 'colsample_bytree': 0.7743707070097846, 'min_child_weight': 8.264701684272053, 'gamma': 10.516991997614205, 'reg_alpha': 7.882830884758379, 'reg_lambda': 1.4163429034878894, 'scale_pos_weight': 1.4465161310149617, 'objective': 'binary:logistic', 'random_state': 1991}\n",
      "Accuracy: 0.84\n",
      "\n",
      "| \u001B[0m9        \u001B[0m | \u001B[0m0.84     \u001B[0m | \u001B[0m0.7744   \u001B[0m | \u001B[0m10.52    \u001B[0m | \u001B[0m0.8362   \u001B[0m | \u001B[0m7.055    \u001B[0m | \u001B[0m8.265    \u001B[0m | \u001B[0m7.883    \u001B[0m | \u001B[0m1.416    \u001B[0m | \u001B[0m1.447    \u001B[0m | \u001B[0m1.0      \u001B[0m |\n",
      "=====================================================================================================================================\n",
      "Wall time: 3.49 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\park sung hyuk95\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# 베이지안 최적화 객체 생성\n",
    "\n",
    "optimizer = BayesianOptimization(f= eval_function, pbounds = param_bounds , random_state= 0)\n",
    "\n",
    "\n",
    "# 베이지안 최적화 수행\n",
    "\n",
    "\n",
    "optimizer.maximize(init_points= 3 , n_iter= 6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "{'colsample_bytree': 0.8646440511781974,\n 'gamma': 10.14556809911726,\n 'learning_rate': 0.610708108550211,\n 'max_depth': 7,\n 'min_child_weight': 7.118273996694524,\n 'reg_alpha': 8.291788226133312,\n 'reg_lambda': 1.275034884505077,\n 'scale_pos_weight': 1.578354600156416,\n 'subsample': 0.9818313802505146,\n 'objective': 'binary:logistic',\n 'random_state': 1991}"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가함수 점수가 최대일 대 하이퍼파라미터\n",
    "max_params = optimizer.max['params']\n",
    "max_params\n",
    "max_params['max_depth'] = int(round(max_params['max_depth']))\n",
    "\n",
    "max_params.update(fixed_params)\n",
    "max_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1 / 폴드 5 ########################################\n",
      "폴드 1  Accuracy: 0.815\n",
      "\n",
      "######################################## 폴드 2 / 폴드 5 ########################################\n",
      "폴드 2  Accuracy: 0.74\n",
      "\n",
      "######################################## 폴드 3 / 폴드 5 ########################################\n",
      "폴드 3  Accuracy: 0.86\n",
      "\n",
      "######################################## 폴드 4 / 폴드 5 ########################################\n",
      "폴드 4  Accuracy: 0.84\n",
      "\n",
      "######################################## 폴드 5 / 폴드 5 ########################################\n",
      "폴드 5  Accuracy: 0.855\n",
      "\n",
      "OOF Accuracy: 0.822\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 층화 K 폴드 교차 검증기 생성\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=1991)\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_val_preds = np.zeros(X.shape[0])\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 테스트 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_test_preds = np.zeros(X_test.shape[0])\n",
    "\n",
    "# OOF 방식으로 모델 훈련, 검증, 예측\n",
    "for idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n",
    "    # 각 폴드를 구분하는 문구 출력\n",
    "    print('#' * 40, f'폴드 {idx+1} / 폴드 {folds.n_splits}', '#' * 40)\n",
    "\n",
    "    # 훈련용 데이터, 검증용 데이터 설정\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "\n",
    "    # XGBoost 모델 훈련\n",
    "    xgb_model = XGBClassifier(**max_params)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # 검증 데이터 예측 확률에 대한 정확도\n",
    "    preds = xgb_model.predict(X_valid)\n",
    "    accuracy = accuracy_score(y_valid, preds)\n",
    "    print(f'폴드 {idx+1}  Accuracy: {accuracy}\\n')\n",
    "\n",
    "    # 모델 성능이 가장 좋을 때의 모델로 테스트 데이터 예측\n",
    "    test_preds = xgb_model.predict(X_test)\n",
    "\n",
    "    # 검증 데이터 예측 확률을 전체 데이터에 저장\n",
    "    oof_val_preds[valid_idx] = preds\n",
    "\n",
    "    # 테스트 데이터 예측 확률을 전체 데이터에 누적하여 저장\n",
    "    oof_test_preds += test_preds\n",
    "\n",
    "# OOF 검증 데이터 예측 확률을 이진 분류로 변환\n",
    "oof_val_preds_binary = np.where(oof_val_preds >= 0.5, 1, 0)\n",
    "\n",
    "# OOF 검증 데이터 정확도 계산\n",
    "oof_accuracy = accuracy_score(y, oof_val_preds_binary)\n",
    "print(f'OOF Accuracy: {oof_accuracy}\\n')\n",
    "\n",
    "# OOF 테스트 데이터 예측 확률의 평균 계산\n",
    "oof_test_preds /= folds.n_splits\n",
    "\n",
    "# OOF 테스트 데이터 예측 확률을 이진 분류로 변환\n",
    "oof_test_preds_binary = np.where(oof_test_preds >= 0.5, 1, 0)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "# OOF 테스트 데이터 예측 확률 저장\n",
    "df_preds = pd.DataFrame(oof_test_preds_binary, columns=['Solution'])\n",
    "df_preds.index += 1  # 인덱스를 1부터 시작하도록 수정"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "      Solution\nId            \n1            1\n2            0\n3            0\n4            0\n5            1\n...        ...\n8996         1\n8997         1\n8998         1\n8999         0\n9000         1\n\n[9000 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Solution</th>\n    </tr>\n    <tr>\n      <th>Id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8996</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8997</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8998</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8999</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9000</th>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>9000 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preds.index.name= 'Id'\n",
    "df_preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 날짜 및 시각 :  [6, 16, 1, 12, 43]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# 현재 날짜와 시각 구하기\n",
    "now = datetime.datetime.now()\n",
    "date_list = [now.month, now.day, now.hour, now.minute, now.second]\n",
    "# 현재 날짜와 시각 출력하기\n",
    "print(\"현재 날짜 및 시각 : \", date_list)\n",
    "\n",
    "# 제출 파일 생성\n",
    "\n",
    "# submission['Survived'] = y_preds\n",
    "df_preds.to_csv(f'XGBClassifier_{date_list[0]}_{date_list[1]}_{date_list[2]}_{date_list[3]}_{date_list[4]}.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}